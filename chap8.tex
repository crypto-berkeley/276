\chapter{Chapter 8}
\label{chap:8}

\\chapter{Proving Computation Integrity}
\\section{Zero-Knowledge Proofs}
Traditional Euclidean style proofs allow us to prove veracity of statements to others. However, such proof systems have two shortcomings: (1) the running time of the verifier needs to grow with the length of the proof, and (2) the proof itself needs to be disclosed to the verifier. In this chapter, we will provide methods enabling provers to prove veracity of statements of their choice to verifiers while avoiding the aforementioned limitations. In realizing such methods we will allow the prover and verifier to be probabilistic and also allow them to interact with each other.\\footnote{Formally, they can be modeled as interactive PPT Turing Machines.}

\\section{Interactive Proofs}
\\begin{definition} {\\normalfont\\textbf{(Interactive Proof System)}} For a language L we have an \\textit{interactive proof system} if \$\\exists\$ a pair of algorithms (or better, interacting machines) \$(\\mathcal{P},\\mathcal{V})\$, where \$\\mathcal{V}\$ runs in polynomial time in its input length, and both can flip coins, such that:
		\\begin{itemize}
			\\item Completeness: \$\\forall x\\in L\$
		\$\$\\Pr_{\\mathcal{P},\\mathcal{V}} \\left[Output_{\\mathcal{V}}(\\mathcal{P}(x) \\leftrightarrow \\mathcal{V}(x))=1\\right]=1,\$\$
			\\item Soundness: \$\\forall x\\notin L\$, \$\\forall \\mathcal{P}^*\$ (unbounded)
		\$\$\\Pr_{\\mathcal{V}} \\left[Output_{\\mathcal{V}}(\\mathcal{P}^*(x) \\leftrightarrow \\mathcal{V}(x))=1\\right]<\\mathsf{negl}(|x|),\$\$
		\\end{itemize} where \$Output_{\\mathcal{V}}(\\mathcal{P}(x) \\leftrightarrow \\mathcal{V}(x))\$ denotes the output of \$\\mathcal{V}\$ in the interaction between \$\\mathcal{P}\$ and \$\\mathcal{V}\$ where both parties get \$x\$ as input.
		We stress that \$\\mathcal{P}\$ and \$\\mathcal{P}^*\$ can be computationally unbounded. 
  \\end{definition}
We can also consider other variants of this definition, e.g. imperfect completeness.

To understand the above definition, let's consider two languages over a pair of graphs \$G_0\$ and \$G_1\$: 
\\begin{enumerate}
	\\item Graph Isomorphism (GI): We say that two graphs \$G_0\$ and \$G_1\$ are isomorphic, denoted \$G_0 \\cong G_1\$, if \$\\exists\$ an isomorphism \$f: V(G_0) \\rightarrow V(G_1)\$ s.t. \$(u,v)\\in E(G_0)\$ iff \$(f(u),f(v))\\in E(G_1)\$, where \$V(G)\$ and \$E(G)\$ are the vertex and edge sets of some graph \$G\$. Let \$GI=\\lbrace(G_0,G_1)|\\  G_0\\cong G_1\\rbrace\$ be the language that consists of pairs of graphs that are isomorphic.
	\\item Graph Non-Isomorphism (GNI): On the other hand, \$G_0\$ and \$G_1\$ are said to be non-isomorphic, \$G_0 \\ncong G_1\$, if \$\\nexists\$ any such \$f\$, and let \$GNI=\\lbrace(G_0,G_1)|\\  G_0\\ncong G_1\\rbrace\$ be the language that consists of pairs of graphs that are not isomorphic.
\\end{enumerate}
 
\\paragraph{Trivial Case of Graph Isomorphism (GI).} A prover can easily prove to a verifier that two graphs are isomorphic by directly providing the isomorphism \$f\$ between them. The verifier can confirm the isomorphism in time polynomial in the size of the graphs (i.e., its input); hence we have perfect completeness. If the graphs are not isomorphic, no isomorphism exists, and the verifier always rejects; we have perfect soundness too. This proof was trivial, and we didn't even require (back-and-forth) interaction. We now look at a more interesting case of GNI. Moreover, looking ahead, we will see more interesting properties that we can ask of proof systems, like zero-knowledge, where this trivial proof system terribly fails, and we will revisit the GI problem to see how we can prove it with zero-knowledge.

\\paragraph{Interactive Proof for Graph Non-Isomorphism (GNI).}  Unlike the case of GI, for GNI, there is no succinct (e.g., linear in the size of graphs) information that the prover can provide, and consequently, no ``efficient'' (polynomial time in the graphs) verification that the verifier can do. This is where the {\\em power of interaction} comes in. In other words, since GNI is not believed to have short proofs, an {\\em interactive} proof could offer the prover a mechanism to prove to a polynomially bounded verifier that two graphs are non-isomorphic. We will now describe an interactive proof system for GNI.

The intuition is simple. Consider a verifier that randomly renames the vertices of one of the graphs and give it to the prover. Can the prover, given the relabeled graph, figure out which graph did the verifier start with?  If \$G_0\$ and \$G_1\$ were not isomorphic, then an unbounded-time prover can figure this out. However, in case \$G_0\$ and \$G_1\$ {are} isomorphic, then the distributions resulting form random relabelings of \$G_0\$ and \$G_1\$ are actually identical. Therefore, even an unbounded prover has no way of distinguishing which graph the verifier started with. So the prover has only a \$\\frac12\$ probability of guessing which graph the verifier started with. Note that by repeating this process we can reduce the success probability of a cheating prover to negligible\\footnote{This strategy is called soundness amplification by ``sequential'' repetition. Later, we might cover proof systems where we additionally consider ``parallel'' repetition to achieve different security properties.}. More formally, given a claim \$(G_0,G_1)\\in GNI\$, we define the following interactive proof system:

%		If they consistently answer correctly, however, it would be hard to remain skeptical against \$G_0 \\ncong G_1\$ as they beat the odds to almost impossible limits.  And so this interaction can ``prove" very strongly to the verifier that \$(G_0,G_1)\\in\$ GNI.  Consider the protocol we can define from this:

		\\begin{center}
			\\includegraphics[scale=.51094]{figures/GNI_IP_Protocol.png}
		\\end{center}

		\\begin{itemize}
			\\item Completeness: If \$(G_0,G_1)\\in\$ GNI, then the unbounded \$\\mathcal{P}\$ can distinguish isomorphism of \$G_0\$ against those of \$G_1\$ and can always return the correct \$b'\$.  Thus, \$\\mathcal{V}\$ will always output 1 for this case.
			\\item Soundness: If \$(G_0,G_1)\\notin\$ GNI, then it is equiprobable that \$H\$ is a random isomorphism of \$G_0\$ as it is of \$G_1\$, and so \$\\mathcal{P}\$'s guess for \$b'\$ can be correct only with a probability \$\\frac{1}{2}\$\\footnote{A curious reader might notice that the challenge bit \$b\$ sampled by \$\\mathcal{V}\$ is information-theoretically hidden from \$\\mathcal{P}\$ (hidden in \$H\$) when \$\\mathcal{P}\$'s claim is false. This is similar to what we saw in Hash Proof Systems before.}.  Repeating this protocol \$k\$ times, with fresh verifier randomness each time, means the probability of guessing the correct \$b'\$ for all \$k\$ interactions is \$\\frac{1}{2^k}\$.  And so the probability of \$\\mathcal{V}\$ outputting \$0\$ (e.g. rejecting \$\\mathcal{P}\$'s proof at the first sign of falter) is \$1-\\frac{1}{2^k}\$.  

		\\end{itemize}

		%The interaction between prover and verifier captures the notion of a proof system for GNI, a problem previously not known to have an efficient method of proof.  By interacting, we can prove what seemed impossible to efficiently prove before!
		To conclude, the interactive proof system we described above enabled something that wasn't possible without interaction. 

\\section{Zero Knowledge Proofs}
We saw a crucial difference between GI and GNI: in GI, the prover already holds a succinct proof to back its claim, we call this a ``witness'', while in GNI, no such succinct proof exists (i.e., there is nothing that the prover can directly send to the verifier to back its claim). From this point onwards, we exclusively focus on the languages of the first kind, i.e., where a witness for the claim exists; these languages cover a vast majority of the use-cases of verifiable computation, and are formalized as follows:

\\begin{definition} {\\normalfont\\textbf{(NP-Verifier)}} A language L has an NP-verifier if \$\\exists\$ a verifier \$\\mathcal{V}\$ that is polynomial time in \$|x|\$ such that:
		\\begin{itemize}
			\\item Completeness: \$\\forall x\\in L,\\ \\exists\\ a\\ proof\\ \\pi\\ s.t.\\ \\mathcal{V}(x,\\pi)=1\$
			\\item Soundness: \$\\forall x \\notin L\$, and \$\\forall\$ purported proof \$\\pi\$, we have \$\\mathcal{V}(x,\\pi)=0\$
		\\end{itemize}
  \\end{definition}

		That is, the conventional idea of a proof is formalized in terms of what a computer can efficiently verify.\\smallskip %A language \$L\$ has an NP-verifier if for all claims of statements \$\\in L\$, a proof can be written down that can be ``easily'' and ``rigorously'' verified if and only if a statement is in the language.

	\\noindent \\textit{Keeping the witness private}. The goal of a proof system is for the verifier to learn if the prover's claim is valid or not. Let's focus on what a verifier actually learns at the end of its interaction with the prover. In the trivial GI proof system we saw above, the verifier learns the entire isomorphism --- in other words, the verifier learns {\\em everything} that the prover knew. 
	This is too much leakage. Imagine the prover holding some secret or valuable information (e.g., its secret key) which is leaked to the verifier. This is not desirable. We want the verifier to learn only the validity of the claim, and nothing more. This is where the notion of zero-knowledge comes in.
	For a proof system for a language with an NP-verifier, this translates to the verifier not learning the witness from the prover.
	
	We now revisit the GI problem\\footnote{GI is not NP-complete.} for which an NP-verifier exists, as we saw earlier. Later, we will consider NP-complete languages like graph 3-coloring --- giving us proof systems for all of NP. \\smallskip

	\\noindent \\textit{Hiding witness for Graph Isomorphism}. We will build the ideas for our proof system with zero-knowledge gradually by iterating through a series of straw-man approaches. On the way, we will formally define zero knowledge. 

	When \$G_0\$ and \$G_1\$ are isomorphic, the isomorphism between them would be a \\textit{witness}, \$w\$, to that fact, that can be used in the proof.  The prover doesn't want to reveal the isomorphism, \$w:V(G_0)\\rightarrow V(G_1)\$, that they claim to have.  The prover is comfortable however giving us a ``scrambled'' version, \$\\phi\$, of \$w\$ as long as it doesn't leak any information about their precious \$w\$.  For example, the prover is willing to divulge \$\\phi = \\pi \\circ w\$ where \$\\pi\$ is a privately chosen random permutation of \$|V|=|V(G_0)|=|V(G_1)|\$ vertices.  Since \$\\pi\$ renames vertices completely randomly, it scrambles what \$w\$ is doing entirely and \$\\phi\$ is just a random permutation of \$|V|\$ elements.  At this point, we might be a little annoyed at the prover since we could have just created a random permutation on our own.  Let's look at why this is still a good starting point.
		
	If we want to be convinced that \$\\phi\$ really is of the form \$\\pi \\circ w\$, thus containing \$w\$ in its definition, and isn't just a completely random permuation, we can note that if it is of that form then \$\\phi(G_0)=\\pi(w(G_0))=\\pi(G_1)\$ (since \$w\$ being an isomorphism implies that \$w(G_0)=G_1\$).  Note that we started with a mapping on input \$G_0\$ and ended with a mapping on input \$G_1\$.  With an isormphism, one could get from one graph to the other seamlessly; if the prover \\textit{really} has the isomorphism it claims to have, then it should have no problem displaying this ability.  So, what if we force the prover to give us \$H=\\pi (G_1)\$ just after randomly choosing its \$\\pi\$ and then let it show us its ability to go from \$G_1\$ to \$G_0\$ with ease: give us a \$\\phi\$ so that \$\\phi(G_0)=\\pi(G_1)=H\$.  The only way the prover can give a mapping that jumps from \$G_0\$ to \$G_1\$ is if they know an isomorphism; in fact, if the prover could find a \$\\phi\$ efficiently but did \\textit{not} know an isomorphism then they would have been able to see that \$\\pi^{-1}(\\phi(G_0))=G_1\$ and thus have \$\\pi^{-1}\\circ\\phi\$ as an isomorphism from \$G_0\$ to \$G_1\$, which would contradict the assumed hardness of finding isomorphisms in the GI problem. So by forcing the prover to give us \$H\$, as we've defined, and to produce a \$\\phi\$ so that \$\\phi(G_0)=H\$, we've found a way to expose provers that don't really have an isomorphism and we can then be convinced that they really do know \$w\$ when they pass our test.  Importantly, the prover didn't directly tell us \$w\$, so we are headed in the right direction.
		
	But not everything is airtight about this interaction.  Why, for instance, would the prover be willing to provide \$H=\\pi(G_1)\$ when they're trying to divulge as little information as possible?  The prover was comfortable giving us \$\\phi\$ since we could have just simulated the process of getting a completely random permutation of vertices ourselves, but couldn't the additional information of \$H\$ reveal information about \$w\$?  At this point, if we look closely, we realize that \$H=\\pi(G_1)=\\pi'(G_0)\$, for some \$\\pi'\$, is just a random isomorphic copy of \$G_0\$ \\textit{and} \$G_1\$ as long as \$G_0 \\cong G_1\$; we could have just chosen a random \$\\pi'\$, set \$H=\\pi'(G_0)\$, and let \$\\phi=\\pi'\$ and would have created our very own random isomorphic copy, \$H\$, of \$G_1\$ that satisfies our test condition \$H=\\phi(G_0)\$, just like what we got from our interaction with the prover. To our annoyance, the prover can easily fool this test. Indeed, the test has a hole in it: how can we force the prover to give us \$H=\\pi(G_1)\$ like we asked?  If the prover is lying and it knows our test condition is to verify that \$H=\\phi(G_0)\$, the prover might just cheat and give us \$H=\\pi(G_0)\$ so it doesn't have to use knowledge of \$w\$ to switch from \$G_1\$ to \$G_0\$.  And, in fact, by doing this and sending \$\\phi=\\pi\$, the prover would fool us!
		
	To keep the prover on their toes, though, we can randomly switch whether we want \$H\$ to equal \$\\phi(G_0)\$ or \$\\phi(G_1)\$.  In our interaction, the prover must first provide \$H=\\pi(G_1)\$ before we let them know which we want. By sending \$H\$, the prover locks itself into a commitment to either \$G_0\$ or \$G_1\$ if it is cheating, but if not, then it can easily move between the two graphs. A prover only has a \$50\\%\$ chance of committing to the same case we want on a given round and so, if they don't have \$w\$ to deftly switch between \$G_0\$ and \$G_1\$ to always answer correctly, they again have to be an extremely lucky guesser if they're trying to lie.
		
	Therefore, we've created an interactive scheme that can catch dishonest provers with probability 1-\$\\frac{1}{2^k}\$ and where we always believe honest provers!
		
		\\begin{center}
			\\includegraphics[scale=.51094]{figures/GI_ZK_Protocol.png}
		\\end{center}
		
		\\begin{itemize}
			\\item Completeness: If \$(G_0,G_1)\\in GI\$ and \$\\mathcal{P}\$ knows \$w\$, then whether \$\\mathcal{V}\$ chooses \$b=0\$ or 1, \$\\mathcal{P}\$ can always give the correct \$\\phi\$ which, by definition, will always result in \$H=\\phi(G_b)\$ and so \$\\mathcal{V}\$ will always output 1.
			\\item Soundness: If \$(G_0,G_1)\\notin GI\$, then \$\\mathcal{P}\$ can only cheat, as discussed earlier, if the original \$H\$ it commits to ends up being \$\\pi(G_b)\$ for the \$b\$ that is randomly chosen at the next step.  Since \$b\$ isn't even chosen yet, this can only happen by chance with probability \$\\frac{1}{2}\$.  And so the probability \$\\mathcal{V}\$ outputs \$0\$ is \$1-\\frac{1}{2^k}\$ for \$k\$ rounds.
		\\end{itemize}
		
		We have just shown that what we have so far is an interactive proof system. We now think of how the notion of zero-knowledge can be formalized here.
		
		As a verifier, we've seen some things in interacting with the prover.  Surely, clever folks like ourselves must be able to glean \\textit{some} information about \$w\$ after seeing enough to thoroughly convince us that the prover knows \$w\$.  We've first seen \$H\$, and we've also seen the random \$b\$ that we chose, along with \$\\phi\$ at the end;  this is our whole view of information during the interaction.  But we're more bewildered than annoyed this time when we realize we could have always just chosen \$b\$ and \$\\phi\$ randomly and set \$H=\\phi(G_b)\$ on our own.  Again, everything checks out when \$G_0 \\cong G_1\$ and we could have produced everything that we saw during the interaction before it even began.  That is, the distribution of the random variable triple (\$H\$, \$b\$, \$\\phi\$) is identical whether it is what we saw from the prover during the interaction or it is yielded from the solitary process we just described.  We've just constructed a complete interactive proof system that entirely convinces us of the prover's knowledge of \$w\$, yet we could have simulated the whole experience on our own!  We couldn't have gained any knowledge about \$w\$ since we didn't see anything we couldn't have manufactured on our own, yet we are entirely convinced that \$(G_0,G_1)\\in\$ GI and that \$\\mathcal{P}\$ knows \$w\$!  And so the prover has proven something to us yet has given us absolutely zero additional knowledge!
		
		This may feel very surprising or as if you've been swindled by a fast talker, and it very much should feel this way; it was certainly an amazing research discovery!  But this is true, and it can be made rigorous, as we do next.
		
		We should first be sure what we want out of this new proof system.  We of course want it to be complete and sound so that we accept proofs iff they're true.  But we also want the verifier to gain zero knowledge from the interaction; that is, the verifier should have been able to simulate the whole experience on its own without the verifier.
		Finally, we would also like all witnesses to a true statement to each be sufficient to prove the veracity of that statement and so we let \$R\$ be the relation s.t. \$x \\in L\$ iff \$\\exists\$ a witness \$w\$ s.t. \$(x,w)\\in R\$.  We can then gather all witness by defining \$R(x)\$ to be the set of all such witnesses. We will first look at a weaker notion of zero-knowledge, called \\textit{Honest Verifier Zero Knowledge} (HVZK), where we only require that an {\\em honest} verifier (follows the protocol steps) does not learn anything from the prover. We will then move on to the stronger notion of \\textit{Zero Knowledge} (ZK), where we extend this to all verifiers, including malicious verifiers.
		
		\\begin{definition} {\\normalfont\\textbf{(Honest Verifier Zero Knowledge Proof [HVZK])}} 
			\$(\\mathcal{P},\\mathcal{V})\$ is a (perfect) HVZK proof system for a language \$L\$ w.r.t. witness relation \$R\$ if 
			\$\\exists\$ a PPT machine \$\\mathcal{S}\$ (called the simulator) s.t. \$\\forall x \\in L\$, \$\\forall w\\in R(x)\$, the following distributions are (identical) indistinguishable:
		\$\$\\{View_{\\mathcal{V}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V}(x))\\} \\approx \\{\\mathcal{S}(x)\\}\$\$
		where \$View_{\\mathcal{V}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V}(x))\$ is the random coins of \$\\mathcal{V}\$ and all the messages \$\\mathcal{V}\$ saw.
  \\end{definition}

\\begin{remark}
In the above definition, \$View_{\\mathcal{V}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V}(x))\$ contains both the random coins of \$\\mathcal{V}\$ and all the messages that \$\\mathcal{V}\$ saw, because they together constitute the view of \$\\mathcal{V}\$, and they are correlated. If the random coins of \$\\mathcal{V}\$ are not included in the definition of \$View_{\\mathcal{V}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V}(x))\$, then even if \$\\mathcal{S}\$ can generate all messages that \$\\mathcal{V}\$ saw with the same distribution as in the real execution, the verifier may still be able to distinguish the two views using its random coins.
\\end{remark}

\\begin{remark}
In the above definition, the order of quantifiers is quite important. We cannot change it to: \$\\forall x \\in L\$, \$\\forall w\\in R(x)\$, \$\\exists\$ a PPT machine \$\\mathcal{S}\$. This is because the definition would be trivially satisfied by hardcoding the witness \$w\$ in the simulator \$\\mathcal{S}\$.
\\end{remark}

To prove HVZK property of the GI proof system we described earlier, we now construct a simulator \$\\mathcal{S}\$, with input \$G_0, G_1\$, as follows:
\\begin{enumerate}
	\\item Sample \$b\\in\\{0,1\\}\$ uniformly at random.
	\\item Sample a random permutation \$\\sigma\$ of the vertices.
	\\item Set \$H \\gets \\sigma(G_b)\$.
	\\item Output \$(H, b, \\sigma)\$.
\\end{enumerate}
It is straightforward to see that this simulator produces the same distribution as the real interaction between the prover and the verifier. This is because \$H = \\sigma(G_b) = \\sigma'(G_{1-b})\$, i.e., \$H\$ is a random permutation of both \$G_0\$ amd \$G_1\$. 
	
To recap: There is an interesting progression of the requirements of a proof system: Completeness, Soundness, and the Zero Knowledge property.  Completeness first cares that a prover-verifier pair exist and can capture all true things as a team that works together; they both honestly obey the protocol trying prove true statements.  Soundness, however, assumes that the prover is a liar and cares about having a strong enough verifier that can stand up to any type of prover and not be misled.  Finally, Zero Knowledge assumes that the verifier is hoping to glean information from the proof to learn the prover's secrets and this requirement makes sure the prover is clever enough that it gives no information away in its proof. Unlike the soundness' requirement for a verifier to combat \\textit{all} malicious provers, HVZK is only concerned with the verifier in the original prover-verifier pair that follows the set protocol. Verifiers that stray from the protocol or cheat, however, are captured in the natural generalization to Zero Knowledge proofs.

\\section{Zero-Knowledge for Graph Isomorphism}

In this section, we construct our final zero-knowledge interactive proof system for GI where we don't have to assume an honest verifier for zero knowledge to hold. The proof system construction is exactly the same as the one we saw earlier. What changes is the definition of zero knowledge, and therefore, the simulator. 

\\begin{definition} {\\normalfont\\textbf{(Zero Knowledge Proof [ZK])}} 
	\$(\\mathcal{P},\\mathcal{V})\$ is a (perfect) ZK proof system for a language \$L\$ w.r.t. witness relation \$R\$ if \$\\forall\$ PPT machines \$\\mathcal{V}^*\$,
	\$\\exists\$ a PPT machine \$\\mathcal{S}\$ (called the simulator) s.t. \$\\forall x \\in L\$, \$\\forall w\\in R(x)\$, the following distributions are (identical) indistinguishable:
\$\$\\{View_{\\mathcal{V^*}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V^*}(x))\\} \\approx \\{\\mathcal{S}(x)\\}\$\$
where \$View_{\\mathcal{V^*}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V^*}(x))\$ is the random coins of \$\\mathcal{V^*}\$ and all the messages \$\\mathcal{V^*}\$ saw.
\\end{definition}
\\begin{remark}
	Note that the order of quantifiers matters again. The definition would be stronger if we switch the order to: \$\\exists\$ a PPT machine \$\\mathcal{S}\$ (called the simulator) s.t. \$\\forall\$ PPT machines \$\\mathcal{V}^*\$. This is because the same simulator would need to work for all possible efficient verifiers. Interestingly, the simulator we construct below for GI satisfies this stronger definition too. In fact, most simulators we know work for all verifiers (i.e. black-box simulators). It wasn't until 2008 that Boaz Barak showed that we can also construct non-black-box simulators. 
\\end{remark}

Recall our protocol for graph isomorphism: the interaction is \$P(x,w) \\leftrightarrow V(x)\$ where \$x\$ represents graphs \$G_0 = (V, E_0)\$ and \$G_1 = (V, E_1)\$ and \$w\$ represents a permutation on \$V\$ such that \$w (G_0) = G_1\$.

\\begin{enumerate}
\\item \$\\mathcal{P}\$ samples a random permutation \$\\sigma: V \\to V\$ and sends the graph \$H = \\sigma(G_1)\$ to \$V\$.

\\item \$\\mathcal{V}\$ samples a random bit \$b\$ and sends it to \$\\mathcal{P}\$.

\\item If \$b = 1\$, then \$\\mathcal{P}\$ defines a permutation \$\\tau\$ to be \$\\sigma\$. If \$b = 0\$, then instead \$\\tau = \\sigma \\circ w\$. \$\\mathcal{P}\$ then sends \$\\tau\$ to \$V\$.

\\item \$\\mathcal{V}\$ verifies that \$\\tau(G_b) = H\$ and accepts if so.

\\end{enumerate}

The reason the simulator for HVZK doesn't work anymore is because a malicious verifier \$\\mathcal{V^*}\$ could pick its bit \$b\$ from a biased distribution (e.g. \$b\$ can be a function of \$H\$ seen by \$\\mathcal{V^*}\$).

For zero knowledge, consider the following simulator\\footnote{The simulator satisfies a stronger ZK property where the same simulator works for all \$\\mathcal{V^*}\$. Refer to the remark above for more details.} \$S\$ with input \$G_0\$ and \$G_1\$ (with vertex set \$V\$) and verifier \$V^*\$:

\\begin{enumerate}
\\item For \$i = 1\\dots T\$; \$T=\\mathsf{poly}(n)\$:
\\begin{enumerate}
	\\item Sample a bit \$b\$ uniformly at random.

	\\item Sample a permutation \$\\sigma: V \\to V\$ uniformly at random
	
	\\item Send \$H = \\sigma (G_b)\$ to \$\\mathcal{V^*}\$.

	\\item Receive \$b'\$ from \$\\mathcal{V^*}\$.

	\\item If \$b=b'\$, then output \$(H, b, \\sigma)\$ and terminate. Otherwise, continue the loop.
\\end{enumerate}
\\item Output \$\\bot\$.

\\end{enumerate}

We construct a sequence of hybrids to prove zero-knowledge. Let \$H_0\$ define the interaction between \$\\mathcal{P}\$ and \$\\mathcal{V^*}\$: \$\\mathcal{P}(x=(G_0, G_1), w)\\leftrightarrow \\mathcal{V^*}(x)\$. Define \$H_1\$ as follows: 
\\begin{enumerate}
	\\item For \$i = 1\\dots T\$:
\\begin{enumerate}
	\\item Sample a bit \$b^*\$ uniformly at random.

	\\item Run \$H_0\$, i.e., \$\\mathcal{P}(x=(G_0, G_1), w)\\leftrightarrow \\mathcal{V^*}(x)\$.
	
	\\item If \$b^*=0\$, output \$View_{\\mathcal{V^*}}(\\mathcal{P}(x,w) \\leftrightarrow \\mathcal{V^*}(x))\$.
	
	\\item If \$b^*=1\$, continue with the loop.
\\end{enumerate}
\\item Output \$\\bot\$.
\\end{enumerate}

The hybrid \$H_1\$ produces identical distribution as \$H_0\$ except if \$b^*\$ in all the \$T\$ iterations is \$0\$; note that \$\\mathcal{P}(x=(G_0, G_1), w)\\leftrightarrow \\mathcal{V^*}(x)\$ doesn't depend on \$b^*\$. This happens with probability at most \$1/2^T\$. Next, we define \$H_2\$ where we change the logic of which transcript is thrown away. In each iteration, if \$b^* = b\$, where \$b\$ is part of the transcript \$\\mathcal{P}(x=(G_0, G_1), w)\\leftrightarrow \\mathcal{V^*}(x)\$, we output the transcript; otherwise, we continue with the loop. Note again that \$b^*\$ is still not used in any of the transcript (interaction). Therefore, distribution produced by \$H_2\$ is identical to \$H_1\$ because the interaction hasn't changed at all. Finally, we construct our last hybrid \$H_3\$ where the simulator \$S\$ is run. The distribution generated by \$H_3\$ is identical to \$H_2\$ by same argument we used for the HVZK simulator. \\bigskip


\\noindent\\textbf{Efficient Provers.} So far, we have considered unbounded provers, but unfortunately (fortunately?), there aren't real-life instances of all-powerful provers that we know of. And for cryptography we must make more reasonable assumptions about the provers.  We will now assume provers are also bounded to be \\emph{efficient}. Note that if the prover in our GI proof system already holds the isomorphism, then generating the proof only takes polynomial time, and it is easy to see that it satisfies the definition below. \\smallskip

\\begin{definition}[Efficient Prover Zero-Knowledge Proof]
We say \$(P, V)\$ is an efficient prover zero-knowledge proof system for a language \$L\$ and relation \$R_L\$ if \\begin{enumerate}

\\item The prover \$P\$ runs in polynomial time.

\\item The protocol is \\emph{complete}. That is, for every \$x \\in L\$ there exists a witness \$w \\in R_L (x)\$ such that \$\$\\Pr [P(x,w) \\leftrightarrow V(x) \\ \\emph{accepts}] = 1.\$\$

\\item The protocol is \\emph{sound} against unbounded provers. That is, for \$\\forall x \\notin L\$, we have \$\$\\Pr [P^*(x,w) \\leftrightarrow V(x) \\ \\emph{rejects}] \\geq 1/2\$\$ for any prover \$P^*\$ of arbitrary computation power and any witness \$w\$.

\\item There exists an expected polynomial time probabilistic machine \$S\$ (a simulator) such that for all PPT \$V^*\$, for all \$x \\in L, w \\in R_L (x), z \\in \\{ 0, 1 \\}^*\$ we have \$\$\\{ View_{V^*} (P(x,w) \\leftrightarrow V^* (x,z)) \\} \\simeq_c \\{ S^{V^*} (x,z) \\} \$\$ \\end{enumerate}

\\end{definition}

The soundness probability can be amplified to be greater than any \$1 - 1/2^k\$, for arbitrary \$k > 0\$, by repeating the proof \$k\$ times. More precisely, we construct an efficient prover zero-knowledge proof system \$(\\tilde P, \\tilde V)\$ which repeats \$(P,V)\$ independently for \$k\$ times, and \$\\tilde V\$ accepts if and only if \$V\$ accepts in all the executions.

It is easy to see that \$\\tilde P\$ runs in polynomial time and that the protocol is complete.
Moreover, it has the following soundness guarantee:
for \$\\forall x \\notin L\$,
\\begin{align*}
& \\Pr \\left[\\tilde P^*(x,w) \\leftrightarrow \\tilde V(x) \\ \\text{rejects}\\right]\\\\
= & 1- \\Pr \\left[\\forall 1\\leq i\\leq k, P^*_i(x,w) \\leftrightarrow V(x) \\ \\text{accepts}\\right] \\\\
= & 1- \\prod_{i=1}^k \\Pr \\left[P^*_i(x,w) \\leftrightarrow V(x) \\ \\text{accepts}\\right] \\\\
\\geq& 1-\\frac{1}{2^k}
\\end{align*}
for any prover \$\\tilde P^*=(P^*_1, \\cdots, P^*_k)\$ of arbitrary computation power and any witness \$w\$.

Finally, it  is zero-knowledge, namely, there exists an expected PPT \$\\tilde S\$ such that for all PPT \$\\tilde V^*\$, and for all \$x \\in L, w \\in R_L (x), z \\in \\{ 0, 1 \\}^*\$,
\$\$\\left\\{ View_{\\tilde V^*} (\\tilde P(x,w) \\leftrightarrow \\tilde V^* (x,z)) \\right\\} \\simeq_c \\left\\{ \\tilde S^{\\tilde V^*} (x,z) \\right\\}.\$\$
The construction of \$\\tilde S\$ is repeating \$S\$ for \$k\$ times. We prove by hybrid argument that the above two distributions are indistinguishable. \$H_i\$ is defined to be the output of repeating \$S\$ for the first \$i\$ executions with \$\\tilde V^*\$ and repeating \$P\$ for the rest \$k-i\$ executions. Then \$H_0\$ is the left distribution and \$H_k\$ is the right one. Any attacher that can distinguish the above two distributions leads to an attacker that can distinguish \$H_{i-1}\$ and \$H_{i}\$ for some \$1\\leq i \\leq k\$, which violates the zero-knowledge property of the original proof system \$(P,V)\$.

Similar to what we saw in previous definitions of zero-knowlege, the order of the quantifiers in item 4 matters.
If we quantify over \$x\$ and \$w\$ before quantifying over the simulator,
then we could hard-code  \$x\$ and \$w\$ into our simulator. That is, for all \$x \\in L, w \\in R_L (x)\$, there exists an expected polynomial time probabilistic machine \$S_{x,w}\$ such that for all PPT \$V^*\$ and \$z \\in \\{ 0, 1 \\}^*\$,
\$\$\\{ View_{V^*} (P(x,w) \\leftrightarrow V^* (x,z)) \\} \\simeq_c \\{ S_{x,w}^{V^*} (x,z) \\} \$\$
Since we would like our simulator to be universal,  this is not acceptable.

If we quantify first over the verifier \$V^*\$ and then over simulators \$S\$, then this variant is considered as \\emph{non-black-box zero-knowledge}. Our standard definition is considered as \\emph{black-box zero-knowledge}. There  also exist variants that use statistical indistinguishability rather than computational indistinguishability.

The \$z\$ in item 4 is considered as \\emph{auxiliary input}. The auxiliary input is crucial for the above argument of soundness amplification.

We will discuss the importance of requiring expected polynomial time in the next section. \\bigskip
%		These are mostly discussed (including auxiliary inputs) in the next class, although the first definition is given below:
%		
%		{\\definition {\\normalfont\\textbf{(Zero Knowledge Proof [ZK])}} For a language L we have a (perfect) \\textit{ZK proof system} w.r.t. witness relation \$R\$ if \$\\exists\$ an interactive proof system, \$(\\mathcal{P},\\mathcal{V})\$ s.t. \$\\exists\$ a PPT machine \$\\mathcal{S}\$ (called the simulator) s.t. \$\\forall x \\in L\$, \$\\forall w\\in R(x)\$, \$\\forall \\mathcal{V}^*\$, the following distributions are identical:
%		\$\$View_{\\mathcal{V}^*}(\\mathcal{P}(x,w) \\leftarrow \\mathcal{V}^*(x))\$\$
%		\$\$\\mathcal{S}^{\\mathcal{V}^*}(x)\$\$
%		where \$\\mathcal{S}^{\\mathcal{V}^*}(x)\$ is the simulator with oracle access to \$\\mathcal{V}^*\$.}


% !TEX root = collection.tex

\\section{Zero-Knowledge for NP}

An \$n\$-coloring of a graph \$G = (A, E)\$ is a function \$c: A \\to \\{1, \\ldots, n \\}\$ such that if \$(i, j) \\in E\$, then \$c(i) \\neq c(j)\$. So we want to paint each vertex of a graph a certain color so that the endpoints of any edge are colored differently.

In the graph 3-coloring problem (3COL), we are given a graph and asked if there exists a 3-coloring. In this section, we will provide a computational zero knowledge proof for 3COL. It is a fact that 3COL is NP-complete, so any problem in NP has a polynomial time reduction to 3COL. Thus, by giving a zero knowledge proof for 3COL, we will show that there are zero knowledge proofs for all of NP.

We will first give a high-level description of a zero-knowledge protocol for 3COL. Suppose a prover \$P\$ wants to convince a verifier \$V\$ that his graph \$G\$ is 3-colorable without revealing what the coloring \$c\$ actually is. If the three colors we use are red, green, and blue, then note that if we colored all the red vertices blue, all the green vertices red, and all the blue vertices green, we would still have a valid 3-coloring. In fact, if \$\\phi\$ was any permutation on the color set of red, green, and blue, then \$\\phi \\circ c\$ would be a valid 3-coloring of \$G\$.

\$P\$ asks \$V\$ to leave the room and then samples a random permutation \$\\phi\$ of the three colors. He colors the vertices of \$G\$ according to \$\\phi \\circ c\$, then covers all the vertices with cups. At this point, \$P\$ invites \$V\$ back into the room. \$V\$ is allowed to pick one edge and then uncover the two endpoints of the edge. If the colors on the two endpoints are the same, then \$V\$ rejects \$P\$'s claim that the graph is 3-colorable.

If the colors on the two endpoints are different, then \$V\$ leaves the room again, \$P\$ samples \$\\phi\$ randomly, and the process repeats itself. Certainly if \$G\$ is actually 3-colorable, then \$V\$ will never reject the claim. If \$G\$ is not 3-colorable, then there will always be an edge with endpoints that are colored identically and \$V\$ will eventually uncover such an edge.

Note that \$V\$ does not gain any information on the coloring because it is masked by a (possibly) different random permutation every time \$V\$ uncovers an edge. Of course this protocol depends on \$P\$ not being able to quickly recolor the endpoints of an edge after removing the cups. This is why we need commitment schemes.

\\subsection{Commitment Schemes}

We want to construct a protocol between a sender and a receiver where the sender sends a bit to the receiver, but the receiver will not know the value of this bit until the sender chooses to "open" the data that he sent. Of course, this protocol is no good unless the receiver can be sure that the sender was not able to change the value of his bit in between when the receiver first obtained the data and when the sender chose to open it.

\\begin{definition}
A \\emph{commitment scheme} is a PPT machine \$C\$ taking input \$(b,r)\$ that satisfies two properties: \\begin{itemize}
\\item (perfect binding) For all \$r, s\$, we have \$C(0,r) \\neq C(1,s)\$.

\\item (computational hiding) \$\\{ C(0, U_n) \\} \\simeq_c \\{ C(1, U_n) \\}\$

\\end{itemize}
\\end{definition}

So for the sender to "open" the data, he just has to send his value of \$r\$ to the receiver. We say that \$r\$ is a \\emph{decommitment} for \$C(x,r)\$. Why do we require perfect binding instead of just statistical binding? If there existed even a single pair \$r, s\$ where \$C(0,r) = C(1,s)\$, then the sender could cheat. If he wished to reveal a bit value of 0 then he could just offer \$r\$ and if he wished to reveal a bit value of 1 then he could just offer \$s\$.

We can use injective one-way functions to construct commitment schemes.

\\begin{theorem}
If injective one-way functions exist, then so do commitment schemes.
\\end{theorem}
\\proof{We can let \$f\$ be an injective one-way function. Recall from Lecture 3 that \$f' (x, r) := (f(x), r)\$ will also be an injective one-way function with hard-core bit \$B(x,r) := \\langle x, r \\rangle\$. We claim that \$C(b,x,r) := (f'(x,r), b \\oplus B(x,r))\$ is a commitment scheme.

If \$(x,r)  \\neq (y,s)\$ then \$C(0,x,r) \\neq C(0,y,s)\$ because \$f'\$ is injective. Since \$C(0,x,r) = (f'(x,r), B(x,r)) \\neq (f'(x,r), \\overline{B(x,r)}) = C(1,x,r)\$, then \$C\$ satisfies perfect binding.

Suppose \$D\$ can distinguish \$C(0, U_n)\$ from \$C(1, U_n)\$. Then we can distinguish \$B(x,r)\$ from \$\\overline{B(x,r)}\$ given \$f'(x,r)\$ which contradicts the fact that \$B(x,r)\$ is a hard-core bit for \$f'(x,r)\$. Thus, \$C\$ has the computational hiding property.}
\\qed

\\medskip
We can extend the definition of commitment schemes to hold for messages longer than a single bit. These commitment schemes will work by taking our commitment schemes for bits and concatenating them together. For the extended definition, we require that for any two messages \$m_0\$ and \$m_1\$ of the same length, the ensembles \$\\{ C(m_0, U_n) \\}\$ and \$\\{ C(m_1, U_n) \\}\$ are computationally indistinguishable.

\\subsection{3COL Protocol}

Below we describe the protocol \$P(x,z) \\leftrightarrow V(x)\$, where \$x\$ describes a graph \$G = (\\{1, \\ldots, n \\}, E)\$ and \$z\$ describes a 3-coloring \$c\$:

\\begin{enumerate}
\\item \$P\$ picks a random permutation \$\\pi : \\{ 1, 2, 3 \\} \\to \\{ 1, 2, 3 \\}\$ and defines the 3-coloring \$\\beta := \\pi \\circ c\$ of \$G\$. Using a commitment scheme \$C\$ for the messages \$\\{ 1, 2, 3 \\}\$, \$P\$ defines \$\\alpha_i = C(\\beta(i), U_n)\$ for each \$i \\in V\$. \$P\$ sends \$\\alpha_1, \\alpha_2, \\ldots, \\alpha_n\$ to \$V\$.

\\item \$V\$ uniformly samples an edge \$e = (i, j) \\in E\$ and sends it to \$P\$.

\\item \$P\$ opens \$\\alpha_i\$ and \$\\alpha_j\$.

\\item \$V\$ will accept only if it received valid decommitments for \$\\alpha_i\$ and \$\\alpha_j\$, and if \$\\beta(i)\$ and \$\\beta(j)\$ are distinct and valid colors.

\\end{enumerate}

It is clear that this protocol is PPT. If \$G\$ is not 3-colorable, then there will be at least a \$1/|E|\$ probability that \$V\$ will reject \$P\$'s claim in step 4. Since \$|E| \\leq n^2\$ we can repeat the protocol polynomially many times to increase the rejection probability to at least 1/2.

We will now show that this protocol is zero-knowledge. We describe a simulator \$S\$ below, given a verifier \$V^*\$: \\begin{enumerate}
\\item Sample an edge \$e = (i, j) \\in E\$ uniformly at random.

\\item Assign \$c_i\$ and \$c_j\$ to have distinct values from \$\\{ 1, 2, 3 \\}\$ and do so uniformly at random. Set \$c_k := 1\$ for all \$k \\neq i, j\$.

\\item Compute \$n\$ random keys \$r_1, \\ldots, r_n\$ and set \$\\alpha_i = C(c_i, r_i)\$ for all \$i\$.

\\item Let \$e' \\in E\$ be the response of \$V^*\$ upon receiving \$\\alpha_1, \\ldots, \\alpha_n\$.

\\item If \$e' \\neq e\$, then terminate and go back to step 1. Otherwise, proceed. If \$S\$ returns to step 1 more than \$2n |E|\$ times, then output \$\\sf{fail}\$ and halt the program.

\\item Print \$\\alpha_1, \\ldots, \\alpha_n, e\$, send \$r_i\$ and \$r_j\$ to \$V^*\$ and then print whatever \$V^*\$ responds with.
\\end{enumerate}

By construction, \$S\$ will run in polynomial time. However, sometimes it may output a \$\\sf{fail}\$ message. We will show that this occurs with negligible probability.

Suppose that for infinitely many graphs \$G\$, \$V^*\$ outputs \$e' = e\$ in step 4 with probability less than \$1/2|E|\$. If this is true, then it is possible for us to break the commitment scheme \$C\$ that we use in \$S\$. Consider a modified version of \$S\$ called \$\\tilde{S}\$, where in step 2 we set \$c_i = 1\$ for all \$i\$. Note that in this case, \$V^*\$ cannot distinguish between any of the edges so the probability that it returns \$e' = e\$ is \$1/|E|\$.

If we gave \$V^*\$ a set of commitments \$\\alpha_k = C(1, r_k)\$ for random keys \$r_k\$, then we would be in the setting of \$\\tilde{S}\$. If we gave \$V^*\$ the commitments \$\\alpha_k\$ but with two of the values set to \$C(c, r)\$ and \$C(c', r')\$ where \$c, c'\$ are distinct random values from \$\\{ 1, 2, 3 \\}\$ and \$r, r'\$ are random keys, then we are in the setting of \$S\$. This implies that it possible to distinguish between these two commitment settings with a probability of at least \$1/2|E|\$ which is non-negligible. It follows that \$V^*\$ outputs \$e' = e\$ with probability less than \$1/2|E|\$ for only finitely many graphs \$G\$.

Thus, the probability that \$S\$ outputs \$\\sf{fail}\$ in the end is less than \$(1 - 1/2|E|)^{2n|E|} < 1/e^n\$ which is negligible.

Now we need to argue that the transcripts generated by \$S\$ are computationally indistinguishable from the transcripts generated by \$P \\leftrightarrow V^*\$. Again, we consider a modified version of \$S\$, called \$S'\$, given a 3-coloring of its input \$G\$ as auxiliary input. In step 2 of the simulation, \$S'\$ will choose a random permutation of the colors in its valid 3-coloring for the values of \$c_i\$ rather than setting all but two values \$c_i\$ and \$c_j\$ equal to 1. Note that this is how our protocol between \$P\$ and \$V\$ behaves.

Observe that \$P \\leftrightarrow V^*\$ is computationally indistinguishable from \$S'\$ because \$S'\$ outputs \$\\sf{fail}\$ with negligible probability. Thus, it suffices to show that \$S\$ and \$S'\$ are computationally indistinguishable. Again, we will suppose otherwise and argue that as a result we can distinguish commitments.

We consider two messages \$m_0\$ and \$m_1\$ of the same length where \$m_0\$ consists of \$n-2\$ instances of the message \$1\$ and two committed colors \$c_i\$ and \$c_j\$ (for a random edge \$(i, j) \\in E\$) and \$m_1\$ consists of a committed random 3-coloring of \$G\$ (with a random edge \$(i, j) \\in E\$) chosen. Observe that by feeding the former message to \$V^*\$ we are in the setting of \$S'\$ and by feeding the latter message to \$V^*\$ we are in the setting of \$S\$. If we could distinguish those two settings, then we could distinguish the commitments for \$m_0\$ and \$m_1\$. This contradiction completes our argument that our 3-coloring protocol is zero-knowledge.
















%\\newcommand{\\st}{~\\text{s.t.}~}
\\newcommand{\\rgets}{\\overset{\\\$}{\\gets}}
%\\newcommand{\\ind}{\\overset{c}{\\approx}}
\\section{NIZK Proof Systems}
We now consider a different class of Zero-Knowledge proof systems, where no
interaction is required: The Prover simply sends one message to the Verifier,
and the Verifier either accepts or rejects. Clearly for this class to be
interesting, we must have some additional structure:
both the Prover and Verifier additionally have access to a common random public string
\$\\sigma\$ (trusted to be random by both). For example, they could derive \$\\sigma\$
by looking at sunspot patterns. 

\\section{Definitions}

\\begin{definition}[NIZK Proof System]
    A \\emph{NIZK proof system} for input \$x\$ in language \$L\$, with witness \$\\omega\$, is a set of
efficient (PPT) algorithms \$(K, P, V)\$ such that:
\\begin{enumerate}
    \\item Key Generation: \$\\sigma \\gets K(1^k)\$ generates the random public string.
    \\item Prover: \$\\pi \\gets P(\\sigma, x, \\omega)\$ produces the proof.
    \\item Verifier: \$V(\\sigma, x, \\pi)\$ outputs \$\\{0, 1\\}\$ to accept/reject the proof.
\\end{enumerate}
Which satisfies the completeness, soundness, and zero-knowledge properties below.
\\end{definition}
Note: We will assume throughout that \$x\$ is of polynomially-bounded length, i.e., we are
considering the language \$L \\cap \\{0, 1\\}^{P(k)}\$.


\\medskip
\\noindent\\textbf{Completeness.} \$\\forall x \\in L, \\forall \\omega \\in R_L(x)\$:
    \$\$\\Pr[\\sigma \\gets K(1^k), \\pi \\gets P(\\sigma, x, \\omega) : V(\\sigma, x,
    \\pi) = 1] = 1.\$\$
There is an alternate definition of Statistical Correctness, where the probability above is \$1 - \\mathsf{negl}(n)\$ instead of \$1\$. For this explanation, though, Completeness will be used.

\\medskip
\\noindent\\textbf{Non-Adaptive Soundness.} \$\\forall x \\not\\in L\$:
    \$\$\\Pr[\\sigma \\gets K(1^k): \\exists~\\pi \\st V(\\sigma, x, \\pi)
    = 1] = \\mathsf{negl}(k).\$\$
If the value of \$\\sigma\$ that is picked is a ``bad" value, then there does not exist a proof \$\\pi\$ for \$x\$ and \$\\sigma\$. The above definition is ``non-adaptive", because it does not allow a cheating
prover to decide which statement to prove after seeing the randomness \$\\sigma\$.
We may also consider the stronger notion of ``adaptive soundness", where the
prover is allowed to decide \$x\$ after seeing \$\\sigma\$:

\\medskip
\\noindent\\textbf{Adaptive Soundness.}
    \$\$\\Pr[\\sigma \\gets K(1^k): \\exists~ (x, \\pi) \\st  x \\not\\in L, V(\\sigma, x, \\pi)
    = 1] = \\mathsf{negl}(k).\$\$
    
\\medskip
\\noindent\\textbf{(Non-Adaptive) Zero-Knowledge.}
    There exists a PPT simulator \$S\$ such that \$\\forall x \\in L, \\omega \\in
    R_L(x)\$, the two distributions are computationally indistinguishable:

\\begin{minipage}{0.5\\textwidth}
    \\begin{enumerate}[itemsep=-3pt]
        \\item \$\\sigma \\gets K(1^k)\$
        \\item \$\\pi \\gets P(\\sigma, x, \\omega)\$
        \\item Output \$(\\sigma, \\pi)\$
    \\end{enumerate}
\\end{minipage}
\\begin{minipage}{0.5\\textwidth}
    \\begin{enumerate}
        \\item \$(\\sigma, \\pi) \\gets S(1^k, x)\$
        \\item Output \$(\\sigma, \\pi)\$
    \\end{enumerate}
\\end{minipage}

\\medskip
That is, the simulator is allowed to generate the distribution of randomness
\$\\sigma\$ together with \$\\pi\$. Note that if we did not allow \$S\$ to produce
\$\\sigma\$, this definition would be trivial (a verifier could convince himself by
running the simulator, instead of interacting with \$P\$). Allowing \$S\$ to
generate \$\\sigma\$ still keeps the definition zero-knowledge (since a verifier sees both \$(\\sigma,
\\pi)\$ together), but puts \$P\$ and \$S\$ on unequal footing.

We could also consider the adaptive counterpart, where a cheating verifier can
choose \$x\$ after seeing \$\\sigma\$:

\\medskip
\\noindent\\textbf{(Adaptive) Zero-Knowledge.}
    There exists a PPT simulator split into two stages \$S_1, S_2\$ such that
    for all PPT attackers \$\\ma\$,
    the two distributions are computationally indistinguishable:

\\medskip
\\begin{minipage}{0.5\\textwidth}
    \\begin{enumerate}[itemsep=0pt]
        \\item \$\\sigma \\gets K(1^k)\$
        \\item \$(x, \\omega) \\gets \\ma(\\sigma)\$, s.t. \$(x, \\omega) \\in R_L\$
        \\item \$\\pi \\gets P(\\sigma, x, \\omega)\$
        \\item Output \$(\\sigma, x, \\pi)\$
    \\end{enumerate}
\\end{minipage}
\\begin{minipage}{0.5\\textwidth}
    \\begin{enumerate}[itemsep=0pt]
        \\item \$(\\sigma, \\tau) \\gets S_1(1^k)\$
        \\item \$(x, \\omega) \\gets \\ma(\\sigma)\$
        \\item \$\\pi \\gets S_2(\\sigma, x, \\tau)\$
        \\item Output \$(\\sigma, x, \\pi)\$
    \\end{enumerate}
\\end{minipage}

\\medskip
\\noindent where \$\\tau\$ should be thought of as local state stored by the simulator (passed
    between stages).

\\bigskip
Now we show that adaptive soundness is not harder to achieve than non-adaptive soundness.
\\begin{theorem}\\label{thm:amplify-soundness}
    Given a NIZK \$(K, P, V)\$ that is \\emph{non-adaptively sound}, we can
    construct a NIZK \$(K', P', V')\$ that is \\emph{adaptively sound}.
\\end{theorem}
\\proof
For \$x_0 \\not\\in L\$, let us call a particular \$\\sigma\$ ``bad for \$x_0\$" if 
there exists a false proof for \$x_0\$ using randomness \$\\sigma\$:
\$\\exists~ \\pi \\st V(\\sigma, x_0, \\pi) = 1\$.
By non-adaptive soundness of \$(K, P, V)\$, we have
\$\\Pr_\\sigma[\\sigma \\text{ bad for } x_0] = \\mathsf{negl}(k)\$.

Now we construct a new NIZK \$(K',P',V')\$ by repeating \$(K,P,V)\$ polynomially-many times
(using fresh randomness, and \$V'\$ accepts if and only if \$V\$ accepts in each iteration).
We can ensure that \$\\mathsf{negl}(k) \\leq 2^{-2P(k)}\$.
Now by union bound:
\$\$
\\Pr[\\sigma \\gets K'(1^k): \\exists~ (x, \\pi) \\st V'(\\sigma, x, \\pi) = 1] \\leq
2^{P(k)}\\cdot \\Pr_\\sigma[\\sigma \\text{ bad for } x_0] \\leq 2^{-P(k)}.\$\$
So this new NIZK is adaptively-sound. \\qed


\\section{NIZKs from Trapdoor Permutations}

\\begin{definition}[Trapdoor One-Way Permutation]
A trapdoor one-way permutation is a collection of one-way permutations \$\\{f_i : D_i \\rightarrow D_i\\}_{i \\in I}\$ where \$D_i \\subset \\{0,1\\}^{|i|}\$ with
five properties.

\\begin{enumerate}
\\item \$\\exists\$ PPT \$G\$ such that \$G(1^k)\$ outputs \$(i,t_i)\$ where \$i \\in I\\cap \\binset{k}\$
\\item It is easy to sample from \$D_i\$ given \$i\$ 
\\item \$f_i\$ is easy to compute but hard to invert
\\item  \$f_i\$ is a permutation
\\item \$\\exists\$ PPT \$A\$ such that \$A(i, y, t_i)\\in f_i^{-1}(y)\$
\\end{enumerate}
\\end{definition}

When \$f_i\$ is a one-way trapdoor permutation, it is a one-way permutation with
the property that it is easy to compute \$f_i^{-1}\$ only if given access to trapdoor
information \$t_i\$. The function \$G\$ is PPT and computes this trapdoor information.
The function \$A\$ is PPT and inverts \$f_i\$ using this trapdoor information.

\\subsection{RSA}

RSA is the only known example of a trapdoor one-way permutation.
It relies on the assumption that factoring numbers is hard,
but testing primality is easy. (It is known that testing primality can be done
deterministically in polynomial time. It is believed that factoring can not be done in
polynomial time, however this has not been proven. The best factoring algorithms
are sub-exponential though.)

\\begin{definition}
RSA defines the functions \$(G, F, A)\$ as follows.
\\begin{align*}
G(1^k) = ((N, e), d) \\,\\,\\text{where} \\,\\,& N=pq, \\,\\, \\text{for primes \$p,q\$},  \\\\
& \\gcd(e,\\phi(N))=1 \\\\
& d = e^{-1} \\pmod{\\phi(N)} \\\\
F_{N,e}(x) = x^e \\pmod{N} & \\\\
A((N,e),y, d) = y^d \\pmod{N} &
\\end{align*}
\\end{definition}

The function \$G\$ randomly selects the values of \$(p, q, e)\$ to satisfy the desired properties. We note that if  \$e\$ were not coprime to \$\\phi(N)\$, then the function would
not be a permutation.

The function \$\\phi\$ is Euler's Totient, and when \$p,q\$ are primes,
\$\\phi(pq) = (p-1)(q-1)\$.
(That is, \$\\phi\$ is the order of the multiplicative group \$\\mathbb{Z}_N\$.)

The trapdoor piece of information is the multiplicative inverse of \$e\$ modulo the order of the
group. It is believed hard to compute this information given only the integer \$N\$.

It is easy to show correctness of this scheme:
\\begin{align*}
A(i, F_i(x), t_i) &= (x^e)^d = x \\pmod{N}
\\end{align*}

We leave it as an exercise that RSA is semantically
secure with no additional assumptions.

\\subsection{NIZK in the Hidden-Bit Model}
The hidden-bit model is a variant of the common-reference-string NIZK,
where the prover can selectively reveal only parts of the random string to the
verifier. (Imagine clouds obscuring the random string in the sky from the
verifier, and the prover can choose which clouds to clear.)

\\newcommand{\\setI}[1]{\\{#1\\}_{i \\in I}}
\\begin{definition}[NIZK in the Hidden-Bit Model]
    A NIZK in the hidden-bit model for statement \$x\$ (with witness \$\\omega\$)
    is efficient algorithms \$(K_H, P_H, V_H)\$
    such that:
\\begin{enumerate}
    \\item \$r \\gets K_H(1^k)\$ generates the hidden random string (\$\\ell\$-bits).
    \\item \$(I, \\phi) \\gets P_H(r, x, \\omega)\$ generates the indices \$I \\subseteq
        [\\ell]\$ to reveal, and the proof \$\\phi\$.
    \\item \$V_H(I, \\setI{r_i}, x, \\phi)\$ accepts or rejects, given the indices \$I\$,
        the random string \$r\$ at indices \$I\$, statement \$x\$, and proof \$\\phi\$.
\\end{enumerate}
Which satisfies the completeness, soundness, and zero-knowledge properties as
previously defined.
\\end{definition}

The above definition is not necessarily very useful in its own right, but it is helpful as a stepping stone toward a more useful construction.

\\begin{theorem}\\label{thm:NIZK-amplify}
    Given a NIZK \$(P_H, V_H)\$ in the hidden-bit model, we can construct a NIZK
    \$(P, V)\$ in the normal model using trapdoor one-way permutations.
\\end{theorem}
\\begin{proof}
    Let the common-reference-string \$\\sigma\$ in the normal model be of length \$k\\ell\$ and  partition it
    into \$\\ell\$ blocks of \$k\$-bits each: \$\\sigma = \\sigma_1\\hdots\\sigma_\\ell\$.
    Let \$\\mathcal{F}\$ be a family of \$2^k\$ trapdoor OWPs, and let \$B(\\cdot)\$
    be the corresponding hard-core bit. We may assume the soundness error
    of \$(P_H, V_H)\$
    (that is, the probability of \$r\$ allowing a fake proof)
    is at most \$2^{-2k}\$, by the same repetition argument as in Theorem~\\ref{thm:amplify-soundness}.
    The protocol for the normal \$(P, V)\$ is:

\\medskip
\\noindent \\textbf{Prover \$P(\\sigma, x, \\omega)\$:}
\\begin{enumerate}
    \\item Sample trapdoor OWP: \$(f, f^{-1}) \\gets \\mathcal{F}(1^k)\$.
    \\item Let \$\\alpha_i = f^{-1}(\\sigma_i)\$ for \$\\forall i \\in [\\ell]\$.
    \\item Compute hidden-bit \$r_i = B(\\alpha_i)\$ for \$\\forall i \\in [\\ell]\$. Let \$r := r_1 \\cdots r_\\ell\$.
    \\item Run the HBM prover: \$(I, \\phi) \\gets P_H(r, x, \\omega)\$.
    \\item Send \$(f, I, \\setI{\\alpha_i}, \\phi)\$ to verifier.
\\end{enumerate}

\\noindent \\textbf{Verifier \$V(\\sigma, x, f, I, \\setI{\\alpha_i}, \\phi)\$:}
\\begin{enumerate}
    \\item Confirm \$f \\in \\mathcal{F}\$, and \$f(\\alpha_i) = \\sigma_i ~\\forall i \\in I\$.
    \\item Compute the revealed bits \$r_i = B(\\alpha_i) ~\\forall i \\in I\$.
    \\item Output \$V_H(I, \\setI{r_i}, x, \\phi)\$.
\\end{enumerate}

Intuitively, \$\\sigma_i\$ hides \$r_i\$ because \$\\sigma_i \\overset{f}{\\gets} \\alpha_i
\\overset{B}{\\to} r_i\$, so by security of the hard-core bit, the verifier cannot
find \$r_i = B(\\alpha_i)\$ from \$\\sigma_i = f(\\alpha_i)\$.

Notice that if the prover is honest, then \$\\alpha_i\$ will be distributed
uniformly random as well (since \$f^{-1}\$ is a permutation), and \$r_i\$ will be
unbiased as well (since \$B(\\cdot)\$ is a hard-core bit). So this reduces exactly
to the HBM distributions, and completeness of this protocol is clear (from completeness of
\$(P_H, V_H)\$).

For soundness: for a fixed \$f = f_0\$, the distribution of \$r_i\$ is
uniformly random, so by the soundness of \$(P_H, V_H)\$ we have
\$\$\\Pr_\\sigma[\\text{\$P^*\$ can cheat using \$f_0\$}] \\leq 2^{-2k}\$\$
However, a cheating \$P^*\$ may be able to cleverly pick \$f\$ to influence \$r_i\$,
allowing him to cheat. Since we know there are only \$2^k\$ possible choices of
\$f\$ (the verifier confirms \$f\$ is properly sampled), we can use the union bound
to prove soundness:

\$\$\\Pr_\\sigma[~\\exists \\text{ some \$f \\in \\mathcal{F}\$ s.t. \$P^*\$ can cheat}] \\leq 2^{k} \\cdot 2^{-2k} = 2^{-k}.\$\$

Note that more serious problems can occur if \$V\$ does not confirm \$f\\in \\mathcal{F}\$. For example, if \$f\$ is not a permutation, then
\$f^{-1}(\\sigma_i)\$ can be multi-valued, and the prover can choose to ``explain"
\$\\sigma_i\$ using either \$\\alpha_i\$ or \$\\alpha_i'\$ -- which is a problem if
\$B(\\alpha_i) \\neq B(\\alpha_i')\$.


To prove zero-knowledge, we construct a sequence of prover-hybrids. The hybrid \$H_0\$ is the ``honest" scenario. Differences from the previous hybrid are in red:

%\\newcommand{\\htitle}[1]{#1\\\\
%\\rule{0.2\\textwidth}{0.4pt}}

\\newcommand{\\htitle}[1]{\\vspace{0.3cm}\$\\overset{#1}{\\underline{\\hspace{5cm}}}\$}
\\newcommand{\\diff}[1]{{\\color{red} #1}}

\\htitle{H_0 \\text{ (normal model)}}
\\begin{enumerate}
    \\item \$\\sigma_1\\hdots\\sigma_\\ell = \\sigma \\xleftarrow{\\\$} \\{0,1\\}^{k\\ell}\$
    \\item \$(f, f^{-1}) \\gets \\mathcal{F}\$
    \\item \$\\alpha_i = f^{-1}(\\sigma_i) ~\\forall i \\in [\\ell]\$
    \\item \$r_i = B(\\alpha_i) ~\\forall i \\in [\\ell]\$
    \\item \$(I, \\phi) \\gets P_H(r, x, \\omega)\$
    \\item Output \$(\\sigma, f, I, \\setI{\\alpha_i}, \\phi)\$
\\end{enumerate}

\\htitle{H_1}
\\begin{enumerate}
    \\item \\diff{\$r_i \\gets \\bit ~\\forall i \\in [\\ell]\$}
    \\item \$(f, f^{-1}) \\gets \\mathcal{F}\$
    \\item \\diff{\$\\alpha_i \\rgets \\{0, 1\\}^k\\ \\text{such that}\\ r_i = B(f^{-1}(\\sigma_i)) ~\\forall i \\in [\\ell]\$}
    \\item \\diff{\$\\sigma_i = f(\\alpha_i) ~\\forall i \\in [\\ell]\$}
    \\item \$(I, \\phi) \\gets P_H(r, x, \\omega)\$
    \\item Output \$(\\sigma, f, I, \\setI{\\alpha_i}, \\phi)\$
\\end{enumerate}
In \$H_1\$, we sample \$\\alpha_i\$ uniformly at random and then generate \$\\sigma_i\$ (instead of
sampling \$\\sigma_i\$ and then generating \$\\alpha_i\$). This induces an exactly identical distribution, since \$f\$
is a permutation.

\\htitle{H_2}
\\begin{enumerate}
    \\item \$r_i \\rgets \\{0, 1\\} ~\\forall i \\in [\\ell]\$
    \\item \$(f, f^{-1}) \\gets \\mathcal{F}\$
    \\item \\diff{\$(I, \\phi) \\gets P_H(r, x, \\omega)\$}
    \\item \\diff{\$\\alpha_i \\rgets B^{-1}(r_i) ~\\forall i \\in [\\ell]\$}
    \\item \$\\sigma_i = f(\\alpha_i) ~\\forall i \\in [\\ell]\$
    \\item Output \$(\\sigma, f, I, \\setI{\\alpha_i}, \\phi)\$
\\end{enumerate}
In \$H_2\$, we again switch the sampling order: first sample the (unbiased) bit
\$r_i\$, then sample \$\\alpha_i\$ from the pre-image of \$r_i\$ (which can be done efficiently
by simply trying random \$\\alpha_i\$'s until \$B(\\alpha_i) = r_i\$).
This distribution is exactly identical to \$H_1\$.
(The sampling order can be thought of as factoring the joint distribution:
\$\\Pr(\\alpha_i, r_i) = \\Pr(r_i)\\Pr(\\alpha_i | r_i)\$)

\\htitle{H_3}
\\begin{enumerate}
    \\item \$(f, f^{-1}) \\gets \\mathcal{F}\$
    \\item \$r_i \\rgets \\{0, 1\\} ~\\forall i \\in [\\ell]\$
    \\item \$\\alpha_i \\rgets B^{-1}(r_i) ~\\forall i \\in [\\ell]\$
    \\item \\diff{\$\\sigma_i = f(\\alpha_i) ~\\forall i \\in I\$}
    \\item \\diff{\$\\sigma_i \\rgets \\{0, 1\\}^k ~\\forall i \\not\\in I\$}
    \\item \$(I, \\phi) \\gets P_H(r, x, \\omega)\$
    \\item Output \$(\\sigma, f, I, \\setI{\\alpha_i}, \\phi)\$
\\end{enumerate}
In \$H_3\$, we only generate \$\\sigma_i\$ honestly for \$i \\in I\$, and output random
\$\\sigma_i\$ for \$i \\not\\in I\$.
To argue that this is computational
indistinguishable from \$H_2\$, first notice that for a fixed (known) bit \$r\$,
\\begin{equation}
\\label{eqn:bit-ind}
\\{f(B^{-1}(r)\\} \\ind \\{f(B^{-1}(\\overline r)\\}
\\end{equation}
where the randomness is over sampling the pre-image \$B^{-1}\$.
Distinguishing the above distributions is by definition equivalent to guessing the
hard-core bit, so they are indistinguishable.
Given the above, we can further argue that
\\begin{equation}
\\label{eqn:rand-ind}
\\{f(B^{-1}(r)\\} \\ind \\mathcal{U}_k
\\end{equation}
where \$\\mathcal{U}_k\$ is uniform over \$\\{0, 1\\}^k\$. To see this, notice
that \$\\mathcal{U}_k\$ can be equivalently generated by first sampling a random
bit \$b\$, then outputting \$f(B^{-1}(b))\$, since \$f\$ is a permutation.
Therefore, any distinguisher for \\eqref{eqn:rand-ind} can also be used to distinguish
\\eqref{eqn:bit-ind} with at least as much distinguishing-advantage (in fact,
twice as much).
Finally, \\eqref{eqn:rand-ind} justifies swapping \$\\sigma_i = f(\\alpha_i) =
f(B^{-1}(r_i))\$ with random for \$i \\not\\in I\$ in hybrid \$H_3\$.





\\htitle{H_4}
\\begin{enumerate}
    \\item \$(f, f^{-1}) \\gets \\mathcal{F}\$
    \\item \\diff{\$(I, \\setI{r_i}, \\phi) \\gets S_H(1^k, x)\$}
    \\item \\diff{\$\\alpha_i \\rgets B^{-1}(r_i) ~\\forall i \\in I\$}
    \\item \$\\sigma_i = f(\\alpha_i) ~\\forall i \\in I\$
    \\item \$\\sigma_i \\rgets \\{0, 1\\}^k ~\\forall i \\not\\in I\$
    \\item Output \$(\\sigma, f, I, \\setI{\\alpha_i}, \\phi)\$
\\end{enumerate}
Finally, \$H_4\$ simply swaps the hidden-bit prover \$P_H\$ for the hidden-bit simulator \$S_H\$,
which is indistinguishable by the zero-knowledge property of \$(P_H, V_H)\$.
So \$(P, V)\$ is a NIZK system in the normal model.
\\qed

\\end{proof}



\\subsection{NIZK in the Hidden-Bit Model for  Graph Hamiltonian}

\\begin{definition}
A Hamiltonian cycle in a graph is a cycle that visits each vertex exactly once. A Hamiltonian graph is a graph that contains a Hamiltonian cycle. More precisely, given a graph
\$G=(V,E)\$ with \$|V|=n\$, we say that \$G\$ is a Hamiltonian graph if there are
\$x_1,\\ldots,x_n\\in V\$ such that they are all distinct vertices, and \$\\forall i\\in\\{1,\\ldots,n-1\\} : (x_i,x_{i+1})\\in E\$,
\$(x_n,x_1)\\in E\$.
\\end{definition}


It is well known that the problem of determining if a graph is Hamiltonian is \$NP\$-complete.
Here we will construct a NIZK proof in the hidden-bit model (HBM) that is able to prove
that a graph is Hamiltonian.

First we define how graphs are represented as matrices.

\\begin{definition}
A graph \$G=(V,E)\$ with \$|V|=n\$, can be represented as an \$n\\times n\$ adjacency matrix \$M_G\$
of boolean values such that
\$M_G[i,j]=\\left\\{
\\begin{array}{l l}
1 & \\text{if \$(i,j)\\in E\$,} \\\\
0 & \\text{otherwise.}
\\end{array}
\\right.\$

A \\emph{cycle matrix} is a matrix which corresponds to a graph that contains a Hamiltonian cycle and
contains no edges other than this cycle.

A \\emph{permutation matrix} is a boolean matrix such that each row and each column has exactly one
entry equal to 1.
\\end{definition}

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=4cm]{figures/cycle.png}
	\\caption{Cycle matrix.}
	\\label{fig:cycle}
\\end{figure}

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=4cm]{figures/permutation.png}
	\\caption{Permutation matrix.}
	\\label{fig:permutation}
\\end{figure}

Every cycle matrix is a permutation matrix, but the converse is not true.
For each size \$n\$, there are \$n!\$ different permutation matrices but
only \$(n-1)!\$ cycle matrices.

In Figure~\\ref{fig:cycle}, one can see the cycle matrix as a cycle \$(1,4,7,6,8,5,3,2)\$
on the set \$\\{1,2,3,4,5,6,7,8\\}\$. In Figure~\\ref{fig:permutation}, it is possible to
interpret the matrix as a permutation \$(1)(2,8,6,5)(3,7,4)\$ on the same set.

\\begin{theorem}
There is a non-interactive zero-knowledge (NIZK) proof in the
hidden-bit model (HBM) for the problem of proving that a graph is Hamiltonian.
\\end{theorem}
\\proof
In the hidden-bit model (HBM), there is a random string \$r\$ with \$\\ell\$ bits that the prover
can read. The prover should be able to produce a proof \$\\phi\$ and choose a set
\$I\\subseteq\\{1,2,\\ldots,\\ell\\}\$ such that the proof and the bits of the string corresponding to
the set \$I\$ will be revealed to the verifier.
\\begin{table}[ht]
\\centering
\\begin{tabular}{r c l}
%\\mright{P}{\\phi,I,\\{r_i \\mid i\\in I\\}}{V}
\\end{tabular}
\\end{table}


Let the graph be \$G=(V,E)\$ with \$|V|=n\$. Note that the content of \$G\$ is public.
The objective is to convince the verifier that the assertion is correct (the graph \$G\$
is Hamiltonian).

Suppose that the random string \$r\$ comes from a distribution
such that this string represents the entries from an \$n\\times n\$ cycle matrix \$M_c\$.
Then a proof can be produced as follows.

Since the prover \$P\$ knows the Hamiltonian cycle \$x_1,\\ldots,x_n\$ in \$G\$, he can find a
function \$\\phi:V\\rightarrow \\{1,2,\\ldots,n\\}\$ that puts the Hamiltonian cycle exactly
over the cycle of \$M_c\$. More precisely, for this function we have
\$M_c[\\phi(x_i),\\phi(x_{i+1})]=1\$ for each edge \$(x_i,x_{i+1})\$ in the Hamiltonian cycle of G
(we view indices modulo \$n\$).
This means that all the edges of \$M_c\$ will be covered by edges of \$G\$. Conversely, all the
non-edges of \$G\$ must be taken to non-edges of \$M_c\$.

So the strategy for the prover is to reveal the mapping \$\\phi\$ and also reveal entries of \$M_c\$ corresponding to \$\\phi(e)\$ where \$e \\notin E\$, so \$e \\in V \\times V \\setminus E\$. More precisely, for the set
\$I=\\{(\\phi(u),\\phi(v)) \\mid (u,v)\\notin E\\}\$, \$P\$ reveals
\$M_c[\\phi(u),\\phi(v)]=0\$,
which proves that \$(\\phi(u),\\phi(v))\$ is a non-edge of \$M_c\$.

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=4cm]{figures/red_blue.png}
	\\caption{Graph matrix that includes a Hamiltonian cycle. Edges are blue/red and the
                 cycle is red. White cells are non-edges.}
	\\label{fig:red_blue}
\\end{figure}

A visual example is shown in Figure~\\ref{fig:red_blue}. The cycle graph \$M_c\$ given by the
random string corresponds to the red cells. These cells have value 1 in the matrix \$M_c\$ and
all other cells have value 0. The prover \$P\$ provides a bijection \$\\phi\$ that
maps the edges of \$G\$ to this matrix in such a way that all red cells are covered and
some others may also be covered (blue cells). The important property guaranteed is that
all the non-edges of \$G\$ are mapped to cells that have a value 0 in the matrix (white cells).

This proof satisfies the three properties required for a zero knowledge proof.

\\textit{Completeness:} if \$P\$ and \$V\$ are both honest, then \$P\$ will be able to convince
\$V\$ that the statement is true. That's because \$P\$ knows the Hamiltonian cycle of \$G\$,
hence he is always able to produce the mapping \$\\phi\$.

\\textit{Soundness:} if \$P\$ is lying and trying to prove a false statement, then he will
get caught with probability 1. If \$P\$ does not know any Hamiltonian cycle in \$G\$, then
any function \$\\phi\$ he chooses will not cover all the edges in \$M_c\$. Hence there will
be an entry in the matrix \$M_c\$ which is one and will be revealed as a non-edge of \$G\$.

\\textit{Zero Knowledge:} \$V\$ cannot get any information besides the fact that \$P\$
knows a Hamiltonian cycle in \$G\$. A simulator \$S\$ for this proof can be simply a machine
that generates a random permutation \$\\phi:V\\rightarrow \\{1,2,\\ldots,n\\}\$ and reveals zeros
for all the non-edges of \$\\phi(G)\$.

\\vspace{5mm}

In this proof we assumed that the random string \$r\$ comes from a very specific distribution
that corresponds to cycle matrices.
Now we need to show that the general problem (where \$r\$ comes from a
random uniform distribution of \$\\ell\$ bits) can be reduced into this previous scenario.

We proceed as follows.
Let the length of the random string be
\$\\ell=\\left\\lceil 3\\cdot \\log_2 n\\right\\rceil \\cdot n^4\$.
We view the random string \$r\$ as \$n^4\$ blocks of \$\\left\\lceil 3\\cdot \\log_2 n\\right\\rceil\$
bits and we generate a random string \$r'\$ of length \$n^4\$ such that each bit in \$r'\$
is 1 if and only if all the bits in the corresponding block of \$r\$ are equal to 1.
This way, the probability that the \$i\$-th bit of \$r'\$ equals 1 is \$\\Pr[r'_i=1]\\approx\\frac{1}{n^3}\$ for every \$i\$.

Then we create an \$n^2\\times n^2\$ matrix \$M\$ whose entries are given by the bits of \$r'\$.
Let \$x\$ be the number of one entries in the matrix \$M\$.
The expected value for \$x\$ is \$\\frac{n^4}{n^3}=n\$.
And the probability that \$x\$ is exactly \$n\$ is noticeable. To prove that, we can use
Chebyshev's inequality:
\$\$\\Pr[|x-n|\\geq n]\\leq\\frac{\\sigma^2}{n^2}=
\\frac{n^4\\cdot \\frac{1}{n^3}\\cdot\\left(1-\\frac{1}{n^3}\\right)}{n^2}<\\frac{1}{n}.\$\$
So we have \$\\Pr[1\\leq x\\leq 2n-1]>\\frac{n-1}{n}\$.
And the probability \$\\Pr[x=k]\$ is maximal for \$k=n\$, so we conclude that
\$\\Pr[x=n]>\\frac{n-1}{n(2n-1)}>\\frac{1}{3n}\$.

Now suppose that this event (\$x=n\$) occurred and we have exactly \$n\$ entries equal to 1
in matrix \$M\$. What is the probability that those \$n\$ entries are all in different rows
and are all in different columns?

We can think about the problem this way: after \$k\$ one entries have been added to the matrix,
the probability that a new entry will be in a different row and different column is given by
\$\\left(1-\\frac{k}{n^2}\\right)^2\$. Multiplying all these values we get

\\begin{align*}
\\Pr[\\text{no collision}] &\\geq \\left(1-\\frac{1}{n^2}\\right)^2 \\cdot \\left(1-\\frac{2}{n^2}\\right)^2
\\cdots \\left(1-\\frac{n-1}{n^2}\\right)^2 \\\\
& > 1 - 2\\left(\\frac{1}{n^2} + \\frac{2}{n^2} +\\cdots + \\frac{n-1}{n^2}\\right)
= 1 - \\frac{n-1}{n} = \\frac{1}{n}.
\\end{align*}

Now assume that this event happened: the matrix \$M\$ has exactly \$n\$ entries equal to 1
and they are all in different rows and different columns.
Then we can define a new \$n\\times n\$ matrix \$M_c\$ by selecting only those \$n\$ rows
and \$n\$ columns of \$M\$. By construction, \$M_c\$ is a permutation matrix.
The probability that \$M_c\$ is a cycle matrix is \$\\frac{(n-1)!}{n!}=\\frac{1}{n}\$.
An example is shown in Figures~\\ref{fig:n2}~and~\\ref{fig:n}.

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=8cm]{figures/n2.png}
	\\caption{Matrix \$M\$ which is \$n^2\\times n^2\$ for \$n=8\$.}
	\\label{fig:n2}
\\end{figure}

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=4cm]{figures/n.png}
	\\caption{Matrix \$M_c\$ which is \$n\\times n\$ for \$n=8\$. The construction worked,
	         because \$M_c\$ is a cycle matrix.}
	\\label{fig:n}
\\end{figure}


Now let's join all those probabilities. The probability that \$M_c\$ is a cycle matrix is at least
\$\$\\frac{1}{3n}\\cdot \\frac{1}{n}\\cdot \\frac{1}{n} > \\frac{1}{3n^3}.\$\$

If we repeat this process \$n^4\$ times, then the probability that \$M_c\$ is a cycle matrix in at least one iteration is at least
\$\$1-\\left(1-\\frac{1}{3n^3}\\right)^{n^4}\\approx 1-e^{-\\frac{n}{3}} = 1-\\mathsf{negl}(n).\$\$


\\bigskip
The proof system works as follows. Given a random string \$r\$, the prover \$P\$ tries
to execute the construction above to obtain a cycle matrix.
If the construction fails, the prover simply reveals all the bits in the string \$r\$
to the verifier, who checks that the constructions indeed fails.
If the construction succeeds, the prover reveals all the entries in the random string \$r\$
that correspond to values in the matrix \$M\$ which are not used in matrix \$M_c\$.
The verifier will check that all these values for matrix \$M\$ are indeed 0.

Then the prover proceeds as in the previous scenario using matrix \$M_c\$: he
reveals the transformation \$\\phi\$ and opens all the non-edges.

This process is repeated \$n^4\$ times. Or, equivalently, a big string of length
\$\\left\\lceil 3\\cdot \\log_2 n\\right\\rceil \\cdot n^4\\cdot n^4\$ is used and they are all
executed together. This produces a zero knowledge proof.

\\textit{Completeness:} if \$P\$ knows the Hamiltonian cycle of \$G\$,
then he will be able to find a suitable transformation \$\\phi\$ whenever a cycle graph is
generated by the construction.

\\textit{Soundness:} if \$P\$ is lying and trying to prove a false statement, then he will
get caught with very high probability. If any of the \$n^4\$ iterations produces a cycle
graph, then \$P\$ will be caught. So the probability that he will be caught is
\$1-e^{-\\frac{n}{3}} = 1-\\mathsf{negl}(n)\$.

\\textit{Zero Knowledge:} again \$V\$ cannot get any information if the construction succeeds.
And if the construction doesn't succeed, all \$V\$ gets is the random string \$r\$, which also
doesn't give any information.
\\qed


\\begin{theorem}
For any language \$L\$ in \$NP\$, there is a non-interactive zero-knowledge (NIZK) proof
in the hidden-bit model (HBM) for the language \$L\$.
\\end{theorem}
\\proof
The language \$L^*\$ of Hamiltonian graphs is \$NP\$-complete. So any problem in \$L\$ can
be reduced to a problem in \$L^*\$. More precisely, there is a polynomial-time function
\$f\$ such that
\$\$x\\in L \\Longleftrightarrow f(x)\\in L^*.\$\$
So given an input \$x\$, the prover can simply calculate \$f(x)\$ and
produce a NIZK proof in the hidden-bit model for the fact that \$f(x)\\in L^*\$.
Then the verifier just needs to calculate \$f(x)\$ and check if the proof for the fact
\$f(x)\\in L^*\$ is correct.
\\qed

\\begin{theorem}\\label{the:NIZK_NP}
For any language \$L\$ in \$NP\$, there is a non-interactive zero-knowledge (NIZK) proof
in the common reference string (CRS) model for the language \$L\$.
\\end{theorem}
\\proof
In Theorem~\\ref{thm:NIZK-amplify} it was shown that any NIZK proof in the hidden-bit model can
be converted into a NIZK proof in the standard (common reference string) model by using
a trapdoor permutation.
\\qed


In this proof we assumed that the random string \$r\$ comes from a very specific distribution
that corresponds to cycle matrices.
Now we need to show that the general problem (where \$r\$ comes from a
random uniform distribution of \$\\ell\$ bits) can be reduced into this previous scenario.

We proceed as follows.
Let the length of the random string be
\$\\ell=\\left\\lceil 3\\cdot \\log_2 n\\right\\rceil \\cdot n^4\$.
We view the random string \$r\$ as \$n^4\$ blocks of \$\\left\\lceil 3\\cdot \\log_2 n\\right\\rceil\$
bits and we generate a random string \$r'\$ of length \$n^4\$ such that each bit in \$r'\$
is 1 if and only if all the bits in the corresponding block of \$r\$ are equal to 1.
This way, the probability that the \$i\$-th bit of \$r'\$ equals 1 is \$\\Pr[r'_i=1]\\approx\\frac{1}{n^3}\$ for every \$i\$.

Then we create an \$n^2\\times n^2\$ matrix \$M\$ whose entries are given by the bits of \$r'\$.
Let \$x\$ be the number of one entries in the matrix \$M\$.
The expected value for \$x\$ is \$\\frac{n^4}{n^3}=n\$.
And the probability that \$x\$ is exactly \$n\$ is noticeable. To prove that, we can use
Chebyshev's inequality:
\$\$\\Pr[|x-n|\\geq n]\\leq\\frac{\\sigma^2}{n^2}=
\\frac{n^4\\cdot \\frac{1}{n^3}\\cdot\\left(1-\\frac{1}{n^3}\\right)}{n^2}<\\frac{1}{n}.\$\$
So we have \$\\Pr[1\\leq x\\leq 2n-1]>\\frac{n-1}{n}\$.
And the probability \$\\Pr[x=k]\$ is maximal for \$k=n\$, so we conclude that
\$\\Pr[x=n]>\\frac{n-1}{n(2n-1)}>\\frac{1}{3n}\$.

Now suppose that this event (\$x=n\$) occurred and we have exactly \$n\$ entries equal to 1
in matrix \$M\$. What is the probability that those \$n\$ entries are all in different rows
and are all in different columns?

We can think about the problem this way: after \$k\$ one entries have been added to the matrix,
the probability that a new entry will be in a different row and different column is given by
\$\\left(1-\\frac{k}{n^2}\\right)^2\$. Multiplying all these values we get

\\begin{align*}
\\Pr[\\text{no collision}] &\\geq \\left(1-\\frac{1}{n^2}\\right)^2 \\cdot \\left(1-\\frac{2}{n^2}\\right)^2
\\cdots \\left(1-\\frac{n-1}{n^2}\\right)^2 \\\\
& > 1 - 2\\left(\\frac{1}{n^2} + \\frac{2}{n^2} +\\cdots + \\frac{n-1}{n^2}\\right)
= 1 - \\frac{n-1}{n} = \\frac{1}{n}.
\\end{align*}

Now assume that this event happened: the matrix \$M\$ has exactly \$n\$ entries equal to 1
and they are all in different rows and different columns.
Then we can define a new \$n\\times n\$ matrix \$M_c\$ by selecting only those \$n\$ rows
and \$n\$ columns of \$M\$. By construction, \$M_c\$ is a permutation matrix.
The probability that \$M_c\$ is a cycle matrix is \$\\frac{(n-1)!}{n!}=\\frac{1}{n}\$.
An example is shown in Figures~\\ref{fig:n2}~and~\\ref{fig:n}.

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=8cm]{figures/n2.png}
	\\caption{Matrix \$M\$ which is \$n^2\\times n^2\$ for \$n=8\$.}
	\\label{fig:n2}
\\end{figure}

\\begin{figure}[ht]
	\\centering
		\\includegraphics[height=4cm]{figures/n.png}
	\\caption{Matrix \$M_c\$ which is \$n\\times n\$ for \$n=8\$. The construction worked,
	         because \$M_c\$ is a cycle matrix.}
	\\label{fig:n}
\\end{figure}


Now let's join all those probabilities. The probability that \$M_c\$ is a cycle matrix is at least
\$\$\\frac{1}{3n}\\cdot \\frac{1}{n}\\cdot \\frac{1}{n} > \\frac{1}{3n^3}.\$\$

If we repeat this process \$n^4\$ times, then the probability that \$M_c\$ is a cycle matrix in at least one iteration is at least
\$\$1-\\left(1-\\frac{1}{3n^3}\\right)^{n^4}\\approx 1-e^{-\\frac{n}{3}} = 1-\\mathsf{negl}(n).\$\$


\\bigskip
The proof system works as follows. Given a random string \$r\$, the prover \$P\$ tries
to execute the construction above to obtain a cycle matrix.
If the construction fails, the prover simply reveals all the bits in the string \$r\$
to the verifier, who checks that the constructions indeed fails.
If the construction succeeds, the prover reveals all the entries in the random string \$r\$
that correspond to values in the matrix \$M\$ which are not used in matrix \$M_c\$.
The verifier will check that all these values for matrix \$M\$ are indeed 0.

Then the prover proceeds as in the previous scenario using matrix \$M_c\$: he
reveals the transformation \$\\phi\$ and opens all the non-edges.

This process is repeated \$n^4\$ times. Or, equivalently, a big string of length
\$\\left\\lceil 3\\cdot \\log_2 n\\right\\rceil \\cdot n^4\\cdot n^4\$ is used and they are all
executed together. This produces a zero knowledge proof.

\\textit{Completeness:} if \$P\$ knows the Hamiltonian cycle of \$G\$,
then he will be able to find a suitable transformation \$\\phi\$ whenever a cycle graph is
generated by the construction.

\\textit{Soundness:} if \$P\$ is lying and trying to prove a false statement, then he will
get caught with very high probability. If any of the \$n^4\$ iterations produces a cycle
graph, then \$P\$ will be caught. So the probability that he will be caught is
\$1-e^{-\\frac{n}{3}} = 1-\\mathsf{negl}(n)\$.

\\textit{Zero Knowledge:} again \$V\$ cannot get any information if the construction succeeds.
And if the construction doesn't succeed, all \$V\$ gets is the random string \$r\$, which also
doesn't give any information.
\\qed


\\begin{theorem}
For any language \$L\$ in \$NP\$, there is a non-interactive zero-knowledge (NIZK) proof
in the hidden-bit model (HBM) for the language \$L\$.
\\end{theorem}
\\proof
The language \$L^*\$ of Hamiltonian graphs is \$NP\$-complete. So any problem in \$L\$ can
be reduced to a problem in \$L^*\$. More precisely, there is a polynomial-time function
\$f\$ such that
\$\$x\\in L \\Longleftrightarrow f(x)\\in L^*.\$\$
So given an input \$x\$, the prover can simply calculate \$f(x)\$ and
produce a NIZK proof in the hidden-bit model for the fact that \$f(x)\\in L^*\$.
Then the verifier just needs to calculate \$f(x)\$ and check if the proof for the fact
\$f(x)\\in L^*\$ is correct.
\\qed

\\begin{theorem}\\label{the:NIZK_NP}
For any language \$L\$ in \$NP\$, there is a non-interactive zero-knowledge (NIZK) proof
in the common reference string (CRS) model for the language \$L\$.
\\end{theorem}
\\proof
In Theorem~\\ref{thm:NIZK-amplify} it was shown that any NIZK proof in the hidden-bit model can
be converted into a NIZK proof in the standard (common reference string) model by using
a trapdoor permutation.
\\qed

\\section{zkSNARKs}
\\section*{Exercises}
\\begin{exercise}[Leaky ZK proof] Formally define:
\\begin{enumerate}
  \\item 
What it means for an  interactive proof \$(P,V)\$ to be \\textbf{first-bit leaky} zero-knowledge, where we require that the protocol doesn't leak anything more than the first bit of the witness.

\\item What it means for an  interactive proof \$(P,V)\$ to be \\textbf{one-bit leaky} zero-knowledge, where we require that the protocol doesn't leak anything more than one bit that is an arbitrary adversarial chosen function of the witness.
    \\end{enumerate}
\\end{exercise}

\\begin{exercise}[Proving OR of two statements] Give a statistical zero-knowledge proof system \$\\Pi = (P,V)\$ (with efficient prover) for the following language.
    \\[ L = \\left\\{((G_0,G_1),(G_0',G_1'))\\left| G_0 \\simeq G_1 \\bigvee G_0' \\simeq G_1'\\right.\\right\\}\\]\\\\
    \\textbf{Caution:} Make sure the verifier doesn't learn which of the two pairs of graphs is isomorphic.
\\end{exercise}

\\begin{exercise} [ZK implies WI] Let \$L \\in NP\$ and let \$(P,V)\$ be an interactive proof system for \$L\$. We say that \$(P,V)\$ is \\emph{witness indistinguishable (WI)} if for all PPT \$V^*\$, for all \$x \\in L\$, distinct witnesses \$w_1, w_2 \\in R_L(x)\$ and  auxiliary input \$z\\in \\binset{*}\$, the following two views are computationally indistinguishable:
\\[View_{V^*} \\left(P(x,w_1) \\leftrightarrow V^*(x,z) \\right) \\simeq_c View_{V^*} \\left(P(x,w_2) \\leftrightarrow V^*(x,z) \\right).\\]
\\begin{enumerate}
\\item Show that if \$(P,V)\$ is an efficient prover zero-knowledge proof system, then it is also witness indistinguishable.

\\item Assume \$(P,V)\$ is an efficient prover zero-knowledge proof system. We have seen in the exercise that \$(P,V)\$ is also witness indistinguishable. Define \$(\\tilde P, \\tilde V)\$ to repeat \$(P,V)\$ independently for \$k\$ times \\emph{in parallel} (\$k\$ is a polynomial), and \$\\tilde V\$ accepts if and only if \$V\$ accepts in all the executions. Prove that \$(\\tilde P, \\tilde V)\$ is still witness indistinguishable.
\\end{enumerate}    
\\end{exercise}

\\begin{exercise}
\\textbf{Multi-statement NIZK.} The NIZK proof system we constructed in class required a fresh common random string (CRS) for each statement proved. In various settings we would like to reuse the same random string to prove multiple theorem statements while still preserving the zero-knowledge property.
    
    A multi-statement NIZK proof system \$(K,P,V)\$ for a language \$L\$ with corresponding relation \$R\$ is a NIZK proof system for \$L\$ with a stronger zero-knowledge property, defined as follows: \$\\exists\$ a PPT machine \$\\mathcal{S} = (\\mathcal{S}_1,\\mathcal{S}_2)\$ such that \$\\forall\$ PPT machines \$A_1\$ and \$A_2\$ we have that:
    \\[\\left|\\Pr\\left[\\begin{split}\\sigma \\gets K(1^\\kappa),\\\\ (\\{x_i,w_i\\}_{i \\in [q]},\\textsf{state}) \\gets A_1(\\sigma),\\\\ \\text{ such that } \\forall i \\in [q], (x_i,w_i)\\in R\\\\\\forall i \\in [q],  \\pi_i \\gets P(\\sigma, x_i,w_i);\\\\
    A_2(\\textsf{state}, \\{\\pi_i\\}_{i \\in [q]}) =1\\end{split}\\right]
    -
    \\Pr\\left[\\begin{split}(\\sigma,\\tau) \\gets \\mathcal{S}_1(1^\\kappa),\\\\ (\\{x_i,w_i\\}_{i \\in [q]},\\textsf{state}) \\gets A_1(\\sigma),\\\\\\text{ such that } \\forall i \\in [q], (x_i,w_i)\\in R\\\\\\forall i \\in [q],  \\pi_i \\gets \\mathcal{S}_2(\\sigma, x_i,\\tau);\\\\ A_2(\\textsf{state}, \\{\\pi_i\\}_{i \\in [q]})=1\\end{split}\\right]\\right|
    \\leq \\textsf{negl}(\\kappa).
    \\]
    
    Assuming that a single statement NIZK proof system \$(K,P,V)\$ for NP exists, construct a multi-statement NIZK proof system \$(K',P',V')\$ for NP.\\\\
\\textbf{Hint:} Let \$g: \\{0,1\\}^\\kappa \\rightarrow \\{0,1\\}^{2\\kappa}\$ be a length doubling PRG. Let \$K'\$ output the output of \$K\$ along with \$y\$, a random \$2\\kappa\$ bit string. To prove \$x \\in L\$ the prover \$P'\$ proves that \$\\exists (w,s)\$ such that either \$(x,w)\\in R\$ or \$y = g(s)\$.
\\end{exercise}


\\paragraph{Answer}
Construction:
\\[ K'(1^n): 
\\begin{array}{l l} 
\\sigma \\leftarrow K(1^{\\kappa}) \\\\  
y \\leftarrow \\{0,1\\}^{2\\kappa} \\\\
\\text{Output } \\sigma' = (\\sigma, y) \\\\
\\end{array} \\]

\\[ P'(\\sigma'=(\\sigma, y), x, w): 
\\begin{array}{l l} 
\\pi \\leftarrow P(\\sigma, (x,y), w)\\text{ if }x \\in L \\text{ or } \\exists s\\text{ s.t. } g(s)=y \\\\
\\text{Output }\\pi \\\\
\\end{array} \\]

\\[ V'(\\sigma', x, \\pi): 
\\begin{array}{l l} 
\\text{Output } V(\\sigma, x, \\pi)
\\end{array} \\]

The completeness of this systems follows from the completeness of \$(K, P, V)\$. The soundness is based on the PRG \$g\$. Since \$y\$ is a randomly chosen string, for almost all possible choice of \$y\$, it is not in the range of \$g\$, and thus \$x \\in L\$ with overwhelming probability. Therefore, the soundness property also follows from that of \$(K,P,V)\$

To prove zero-knowledge property, we construct the simulator \$\\mathcal{S}_1, \\mathcal{S}_2\$ as follows:
\\[ \\mathcal{S}_1(1^\\kappa): 
\\begin{array}{l l} 
\\sigma \\leftarrow K(1^\\kappa) \\\\
s \\leftarrow {0,1}^{n}, y' = g(s) \\\\
\\text{Output } ((\\sigma, y'), s)
\\end{array}
\\text{, }
\\mathcal{S}_2(\\sigma'=(\\sigma, y'), x, \\tau=s): 
\\begin{array}{l l} 
\\text{Output } P(\\sigma, (x,y'), s) \\\\
\\end{array} \\]

We show that the proofs from \$S_2\$ are indistinguishable from those from \$P'\$ via hybrid argument. Consider following sequence of the proofs (\$\\{\\pi_i, i \\in [q]\\}\$).

\\[ 
\\mathcal{H}_0: 
\\begin{array}{l l} 
\\sigma \\leftarrow K(1^n) \\\\
y \\leftarrow \\{0,1\\}^{2n} \\\\
\\{\\pi_i \\leftarrow P(\\sigma, (x_i, y), w_i)\\}
\\end{array} 
\\mathcal{H}_1: 
\\begin{array}{l l} 
\\sigma \\leftarrow K(1^n) \\\\
s \\leftarrow \\{0,1\\}^{n},  y = g(s) \\\\
\\{\\pi_i \\leftarrow P(\\sigma, (x_i, y), w_i)\\}
\\end{array} 
\\mathcal{H}_2: 
\\begin{array}{l l} 
\\sigma \\leftarrow K(1^n) \\\\
s \\leftarrow \\{0,1\\}^{n},  y = g(s) \\\\
\\pi_1 \\leftarrow P(\\sigma, (x_i, y), s) \\\\
\\{\\pi_i \\leftarrow P(\\sigma, (x_i, y), w_i)\\, i \\in [1,q]\\}
\\end{array} 
\\]

Since \$g\$ is PRF, it can be shown that \$\\mathcal{H}_0 \\cong \\mathcal{H}_1\$ using Sunglass Lemma. Next, consider an intermediate hybrid \$\\mathcal{H}_{1.5}\$ where everything is the same as \$\\mathcal{H}_1\$, but the generation of \$\\pi_1\$ is simulated by \$\\mathcal{S}_1, \\mathcal{S}_2\$. \$\\mathcal{H}_1\$ and \$\\mathcal{H}_2\$ are independently indistinguishable from \$\\mathcal{H}_{1.5}\$, \$\\mathcal{H}_1 \\cong \\mathcal{H}_{1.5} \\cong \\mathcal{H}_2\$. This process can be extended the argument to \$\\mathcal{H}_3, \\mathcal{H}_4,...\$ where the next \$\\pi\$s are generated with \$s\$ in the same way. Therefore, by the hybrid argument, \$\\mathcal{H}_1\$ is indistinguishable from a sequence where all proofs are generated with \$s\$. Thus, \$\\mathcal{S}_1, \\mathcal{S}_2\$ simulates the proofs by \$P'\$.



\\subsection{Naor-Yung Construction}

The Naor-Yung construction relies on a semantically-secure public-key encryption scheme \$(\\mathsf{Gen}, \\mathsf{Enc}, \\mathsf{Dec})\$ and an adaptively-secure NIZK proof system \$(K, P, V)\$ to construct a public-key encryption scheme \$(\\mathsf{Gen_{cca1}}, \\mathsf{Enc_{cca1}}, \\mathsf{Dec_{cca1}})\$ that is CCA1 secure. The scheme is defined as follows:

\\begin{itemize}
    \\item \$\\mathsf{Gen_{cca1}}(1^n): \$
    \\begin{enumerate}
        \\item \$(pk_0, sk_0) \\gets \\mathsf{Gen}(1^n)\$
        \\item \$(pk_1, sk_1) \\gets \\mathsf{Gen}(1^n)\$
        \\item \$\\sigma \\gets K(1^n)\$
        \\item Output \$pk = (pk_0, pk_1, \\sigma)\$, \$sk = sk_0\$
    \\end{enumerate}
    \\item \$\\mathsf{Enc_{cca1}}(pk = (pk_0, pk_1, \\sigma), m): \$
    \\begin{enumerate}
        \\item \$\\{c_b \\gets \\mathsf{Enc}(pk_b, m; r_b)\\}_{b\\in\\bit}\$
        \\item \$\\Pi \\gets P(\\sigma, (c_0, c_1), (m, r_0, r_1))\$
        \\item Output \$(c_0, c_1, \\Pi)\$
    \\end{enumerate}
    \\item  \$\\mathsf{Dec_{cca1}}(sk = sk_0, CT = (c_0, c_1, \\Pi)): \$
    \\begin{enumerate}
        \\item If \$V(\\sigma, (c_0, c_1), \\Pi) = 1\$: output \$\\mathsf{Dec}(sk_0, c_0)\$
        \\item Else: output \$\\bot\$
    \\end{enumerate}
\\end{itemize}

\\begin{theorem}\\label{thm:naor-yung-cca1}
    Assuming \$(\\mathsf{Gen}, \\mathsf{Enc}, \\mathsf{Dec})\$ is a semantically-secure encryption scheme and \$(K, P, V)\$ is an adaptively-secure NIZK proof system, then \$(\\mathsf{Gen_{cca1}}, \\mathsf{Enc_{cca1}}, \\mathsf{Dec_{cca1}})\$ is secure against non-adaptive chosen-ciphertext attacks. 
\\end{theorem}
\\proof 
Recall the definition of CCA1 security game:

\\begin{center}\\begin{tabular}{ r c l }
  \\textbf{Challenger} & & \\textbf{Adversary} \\\\
  \$(pk, sk) \\gets \\mathsf{Gen}(1^n)\$ & \$\\xrightarrow{pk = (pk_0, pk_1, \\sigma)}\$ &  \\\\
   & \$\\xleftarrow{CT_1}\$ & \\\\
  \$m_1 \\gets \\mathsf{Dec}(sk, CT_1)\$ & \$\\xrightarrow{m_1}\$ & \\\\
  \$\\vdots\$ & \$\\vdots\$ & \$\\vdots\$ \\\\
   & \$\\xleftarrow{CT_i}\$ & \\\\
  \$m_i \\gets \\mathsf{Dec}(sk, CT_i)\$ & \$\\xrightarrow{m_i}\$ & \\\\
   & \$\\xleftarrow{m_0, m_1}\$ & \\\\
  \$b \\overset{\\\$}{\\gets} \\{0, 1\\}\$, \$c^* = \\mathsf{Enc}(pk, m_b)\$ & \$\\xrightarrow{c^* = (c_0^*, c_1^*, \\Pi^*)}\$ & \\\\
  Output \$1\$ if \$b' = b\$, otherwise \$0\$ & \$\\xleftarrow{b'}\$ & \\\\
\\end{tabular}
\\end{center}

Starting from this Game 0, we can derive Game 1 to 4, so that it is clear that at Game 4, the adversary does not receive any information about \$b\$ from \$c^*\$, and hence it is impossible to correctly guess \$b\$ with a non-negligible advantage. The intuition here is to replace \$c_0^*, c_1^*, \\Pi^*\$ step by step. An important lemma that we make use of in the construction is:
\\begin{lemma} \\label{lemma:naor-yung}
For \$\\forall CT = (c_0, c_1, \\Pi)\$, we have that if \$V(\\sigma, (c_0, c_1), \\Pi) = 1\$, \$\\mathsf{Dec}(sk_0, c_0) = \\mathsf{Dec}(sk_1, c_1).\$
\\end{lemma}

The lemma is a direct result of the soundness guarantee of the NICK proof system. Now we are ready to construct the games:
\\begin{itemize}
    \\item Game 1: instead of \$(P, V)\$, its simulator is used to provide the proof \$\\Pi^*\$. Specifically, we  have \$(\\sigma, \\tau) \\gets S_1(1^n)\$ and \$\\Pi^* \\gets S_2(\\tau, (c_0^*, c_1^*))\$. Now \$\\Pi^*\$ does not contain information about \$b\$.
    \\item Game 2: note that only \$sk_0\$ is used to answer all the decryption queries by the adversary, as a result, we can set \$c_1^*\\gets \\mathsf{Enc}(pk_1, 0^{|m_1|})\$. This works because of the indistinguishability of the public-key encryption scheme. Now \$c_1^*\$ does not contain information about \$b\$.
    \\item Game 3: using Lemma~\\ref{lemma:naor-yung}, we can use \$sk_1\$ instead of \$sk_0\$ to answer all the decryption queries by the adversary.
    \\item Game 4: now, since only \$sk_1\$ is used to answer all the decryption queries by the adversary. we can set \$c_0^*\\gets \\mathsf{Enc}(pk_0, 0^{|m_0|})\$. This works because of the indistinguishability of the public-key encryption scheme. Now \$c_0^*\$ does not contain information about \$b\$ either.
\\end{itemize}



\\section{zkSNARKs}

In this section, we will overview the fundamentals of zkSNARKs, or Zero-Knowledge Succinct Non-interactive ARguments of Knowledge.

\\subsection{Preliminaries}

Before diving into zkSNARKs, let's understand the basic framework we're working with. In cryptography, we often deal with statements of the form ``I know a secret value that satisfies some property.'' For instance, we might want to prove that we know the private key corresponding to a public key, or that we know a solution to a Sudoku puzzle, or that we know a valid password for an account.

To formalize these statements, we use what's called a binary relation \$R\$. This relation takes two inputs: the public statement \$x\$ (like a Sudoku puzzle) and the secret witness \$w\$ (like the solution). \$R\$ is an efficiently computable binary relation that outputs 1 if the witness \$w\$ is valid for statement \$x\$, and 0 otherwise.

For an NP language \$\\mathcal{L}\$, we can say that \$x \\in \\mathcal{L}\$ if and only if there exists a witness \$w\$ such that \$R(x, w) = 1\$ (\$R\$ being the corresponding to \$\\mathcal{L}\$ relation). Conversely, \$x \\notin \\mathcal{L}\$ if and only if there does not exist any witness \$w\$ such that \$R(x, w) = 1\$. A crucial property is that while finding a valid witness \$w\$ may be computationally hard (like solving a Sudoku puzzle), verifying the relation \$R(x,w)=1\$ is always efficient (like checking if a Sudoku solution is valid). This verification can be done in polynomial time.

This framework allows us to express a wide variety of practical problems where we want to prove knowledge of a solution without revealing the solution itself, which is exactly what zkSNARKs will help us achieve.

\\subsection{Properties of zkSNARKs}

We now introduce the properties of zkSNARKs. We seek to use zkSNARKs to prove that \$x \\in L \\iff R(x, w) = 1\$. Informally, if a prover has \$(x, w)\$, we must send a proof \$\\pi\$ such that the verifier, who has the instance \$x\$, can efficiently check that \$R(x, w) = 1\$. For the purposes of this lecture, we are focusing on non-interactive proof systems. This means that there is no back-and-forth communication between the prover and the verifier, and the verifier can verify the prover's statement in one shot.

Two of the properties of zkSNARKs are common to all proof systems: Correctness ensures that if the statement is true, the prover should always be able to convince the verifier. Soundness ensures that no cheating prover can convince the verifier about an invalid statement. However, a correct and sound proof system is not particularly interesting. Indeed, we could simply achieve this by sending the witness \$w\$ from the prover to the verifier. We now discuss two properties that make zkSNARKs interesting: Succinctness and Zero-Knowledge. Succinctness requires that the proof \$\\pi\$ sent by the prover is significantly smaller than the witness \$w\$. Specifically, the proof size \$|\\pi|\$ must be bounded by \$poly(\\lambda, \\log(|w|))\$, where \$\\lambda\$ is the security parameter. This means the proof grows only (at most) polylogarithmically with the witness size. Zero-knowledge ensures that the proof should not reveal any information about the witness \$w\$ beyond what can be deduced from the statement being proven. This property is what differentiates zkSNARKs from SNARKs.

The succinctness properties make SNARKs incredibly relevant, even for practical applications where we do not care about hiding the witness \$w\$. This is because the proof size is exponentially more efficient than directly sending the witness \$w\$ from the prover to the verifier.

\\paragraph{Succinctness example.} Let's consider a practical example to illustrate the power of SNARKs' succinctness property. Suppose we have a 1 TB hard disk and want to prove to a verifier that \$Hash(hard\\text{ }disk) = x\$ for some known hash value \$x\$. We have two options: Without SNARKs, we would need to send the entire 1 TB hard disk to the verifier, who then computes the hash themselves. With SNARKs, we can generate a succinct proof \$\\pi\$ (for example 1 KB) that proves knowledge of the hard disk contents whose hash equals \$x\$. Even in scenarios where we do not need to hide the hard disk contents (i.e., zero-knowledge is not required), the SNARK approach is dramatically more efficient in terms of communication complexity: sending a 1 KB proof versus transferring 1 TB of data. This lecture will primarily focus on achieving succinctness, as typically adding zero-knowledge properties is a relatively straightforward extension once the basic SNARK construction is understood.

\\paragraph{How to build SNARKs?} To construct SNARKs, we will follow these key steps. First, we need to find a ``SNARK-friendly'' representation model for NP languages. The example we will use is square span programs. Next, we must show that this model can capture all NP relations, using boolean circuits as an example. Then, we construct the SNARK using cryptographic techniques, specifically employing bilinear groups in our example construction. After that, we prove soundness. Finally, we add zero-knowledge properties. We will study the DFGK14 SNARK, which conceptually is in the SNARK family of the widely known Groth16 zkSNARK. DFGK14 which is conceptually simpler but employs the same fundamental cryptographic ideas.

\\subsection{Square Span Programs}

We begin by introducing Square Span Programs (SSPs), which provide a ``SNARK-friendly'' representation for NP languages.

\\begin{definition}[Square Span Program]
A square span program \$Q\$ over a field \$\\mathbb{F}\$ consists of a size parameter \$m \\in \\mathbb{N}\$, a degree parameter \$d \\in \\mathbb{N}\$, and a set of polynomials \$\\{v_0(x), v_1(x), \\dots, v_m(x), t(x)\\}\$. Each \$v_i(x)\$ is a polynomial over \$\\mathbb{F}\$ of degree at most \$d\$, and \$t(x)\$ is a polynomial over \$\\mathbb{F}\$ of degree exactly \$d\$.
\\end{definition}

\\begin{definition}[SSP Acceptance]
We say that a square span program \$Q\$ accepts an input \$(a_1, \\dots, a_\\ell) \\in \\mathbb{F}^\\ell\$ if and only if there exist values \$a_{\\ell+1}, \\dots, a_m \\in \\mathbb{F}\$ such that \$t(x)\$ divides \$(v_0(x) + \\sum_{i=1}^m a_i v_i(x))^2 - 1\$. In other words, there exists a quotient polynomial \$h(x)\$ such that \$(v_0(x) + \\sum_{i=1}^m a_i v_i(x))^2 - 1 = h(x)t(x)\$.
\\end{definition}

The values \$a_1, \\dots, a_\\ell\$ represent the input to our computation, while \$a_{\\ell+1}, \\dots, a_m\$ serve as auxiliary values (similar to a witness in an NP relation). As we will see, this algebraic structure is particularly well-suited for constructing SNARKs. A key property of SSPs is their expressiveness: we can transform any boolean circuit into an equivalent square span program. This transformation will be our next focus.

\\subsection{From Boolean Circuits to SSPs}

Consider the boolean circuit in Figure \\ref{fig:ssp}. We will transform this circuit into an SSP.

\\begin{figure}[h]
  \\centering
  \\includegraphics[width=0.4\\textwidth]{figures/ssp.pdf}
  \\caption{Boolean circuit example for SSP transformation \\label{fig:ssp}} 
\\end{figure}

First, we express each gate type as a linear constraint. For a XOR gate, the output \$a_4\$ of inputs \$a_1, a_2\$ must satisfy \$a_1 + a_2 + a_4 \\in \\{0, 2\\}\$. For an OR gate, the output \$a_5\$ of inputs \$a_2, a_3\$ must satisfy \$(1-a_2) + (1-a_3) - 2(1-a_5) \\in \\{0, 1\\}\$. For a NAND gate, the output \$a_6\$ of inputs \$a_4, a_5\$ must satisfy \$a_4 + a_5 - 2(1-a_6) \\in \\{0, 1\\}\$.

For the circuit to be satisfiable, all gate constraints must be satisfied, the output should be \$1\$ or equivalently the constraint \$3(1 - a_6) \\in \\{0, 2\\}\$ must hold, and all boolean value constraints \$a_i \\in \\{0, 1\\}\$ for all \$i \\in \\{1,\\ldots,6\\}\$ must be met. To standardize these constraints, we multiply all constraints involving \$\\{0, 1\\}\$ by 2 to normalize ranges and combine constraints into a matrix form for \$a_1,\\ldots,a_6\$. Finally, we seek to unify the constraints into a larger matrix that encompasses all the constraints in the whole circuit, where each column represents a single boolean algebra constraint. The resulting constraint matrix \$M\$ is below, with columns representing the XOR, OR, NAND, and output constraints, respectively:

\\[ \\begin{pmatrix}
1 & 0 & 0 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\\\
1 & -2 & 0 & 0 & 0 & 2 & 0 & 0 & 0 & 0 \\\\
0 & -2 & 0 & 0 & 0 & 0 & 2 & 0 & 0 & 0 \\\\
1 & 0 & 2 & 0 & 0 & 0 & 0 & 2 & 0 & 0 \\\\
0 & 4 & 2 & 0 & 0 & 0 & 0 & 0 & 2 & 0 \\\\
0 & 0 & 4 & -3 & 0 & 0 & 0 & 0 & 0 & 2
\\end{pmatrix} \\]

There is also a constant vector \$\\vec{\\delta}\$ associated with these constraints:

\\[ \\vec{\\delta} = [0 \\; 0 \\; -4 \\; 3 \\; | \\; 0 \\; 0 \\; 0 \\; 0 \\; 0 \\; 0] \\]

All constraints must evaluate to elements in \$\\{0, 2\\}^{10}\$. 

\\paragraph{Generalizing to Arbitrary Circuits.}

Let's now see how we can transform any arbitrary boolean circuit (with fan-in \$2\$, fan-out \$1\$ gates) into a Square Span Program. Consider a circuit with \$m\$ wires and \$n\$ gates. Our first task is to construct a matrix that captures all the constraints of our circuit. For each gate \$k\$ in our circuit, we create a column vector \$\\vec{v_k} = (v_{1k}, v_{2k}, \\ldots, v_{mk})^T\$ that encodes the gate's constraints. These constraints ensure the gate operates correctlyjust as we saw with our XOR, OR, and NAND gates in the previous example.

After encoding all \$n\$ gates, we add a special column for the output constraint, which takes the form \$(0,\\ldots,0,-3)^T\$. The \$-3\$ here is somewhat arbitrary; any field element different from 2 would work. We then augment our matrix with a diagonal matrix \$D\$ where each diagonal entry is 2. This diagonal matrix serves to enforce that all our variables are boolean, a crucial requirement for circuit satisfaction.

To complete our constraint system, we need a constant vector \$\\vec{\\delta} = (\\delta_1,\\ldots,\\delta_n,3,0,\\ldots,0)\$ where each \$\\delta_i\$ represents the constant term for gate \$i\$. These constants are chosen from the set \$\\{0,2\\}\$, with the exception of the output constraint's constant which is 3.

Let's now see how we can transform any arbitrary boolean circuit into a Square Span Program. Consider a circuit with \$m\$ wires and \$n\$ gates. The complete constraint system can be written as:

\\[ \\begin{pmatrix} a_1 & a_2 & \\cdots & a_m \\end{pmatrix}
\\begin{pmatrix}
v_{11} & v_{12} & \\cdots & v_{1n} & 0 & 2 & 0 & \\cdots & 0 \\\\
v_{21} & v_{22} & \\cdots & v_{2n} & 0 & 0 & 2 & \\cdots & 0 \\\\
v_{31} & v_{32} & \\cdots & v_{3n} & 0 & 0 & 0 & \\ddots & 0 \\\\
\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\
v_{m1} & v_{m2} & \\cdots & v_{mn} & -3 & 0 & 0 & \\cdots & 2
\\end{pmatrix} + \\begin{pmatrix} \\delta_1 & \\delta_2 & \\cdots & \\delta_n & 3 & 0 & \\cdots & 0 \\end{pmatrix} \\]

where \$(a_1,\\ldots,a_m)\$ are the wire values (variables we solve for), the first \$n\$ columns \$(v_{ij})\$ represent the gate constraints, column \$n+1\$ is the output constraint \$(0,\\ldots,0,-3)^T\$, the next \$m\$ columns form the diagonal matrix \$D\$ with \$2\$'s on the diagonal, and the constant vector \$(\\delta_1,\\ldots,\\delta_m)\$ contains \$\\delta_i \\in \\{0,2\\}\$ for \$i \\leq n\$ (gate constraints), \$\\delta_{n+1} = 3\$ (output constraint), and \$\\delta_i = 0\$ for \$i > n+1\$ (boolean constraints).

Next, we convert these discrete constraints into a polynomial system. We choose distinct field elements \$x_1,\\ldots,x_d\$ (where \$d=n+1+m\$ is our total number of constraints) and use polynomial interpolation to create our SSP. For each row \$i\$ of our matrix, we construct a polynomial \$v_i(x)\$ that evaluates to the \$(i,j)\$ entry when evaluated at point \$x_j\$. Similarly, we create a polynomial \$v_0(x)\$ that interpolates our constant vector \$\\vec{\\delta}\$.

The target polynomial \$t(x)\$ is defined as the product of all linear terms:

\\[ t(x) = \\prod_{j=1}^d (x-x_j) \\]

This polynomial is crucial because it ``zeros out'' exactly at our constraint points. The beauty of this construction is that it transforms our circuit satisfaction problem into an elegant polynomial divisibility question: the circuit is satisfiable if and only if there exist values \$(a_1,\\ldots,a_m)\$ such that:

\\[ \\left(v_0(x) + \\sum_{i=1}^m a_i v_i(x)\\right)^2 - 1 = h(x)t(x) \\]

for some polynomial \$h(x)\$.

After converting to a polynomial system, our constraint matrix becomes:

\\[ \\begin{pmatrix} 
a_1 \\\\ 
a_2 \\\\ 
\\vdots \\\\ 
a_m 
\\end{pmatrix}^T
\\begin{pmatrix}
v_1(x_1) & v_1(x_2) & \\cdots & v_1(x_n) & 0 & v_1(x_{n+2}) & 0 & \\cdots & 0 \\\\
v_2(x_1) & v_2(x_2) & \\cdots & v_2(x_n) & 0 & 0 & v_2(x_{n+2}) & \\cdots & 0 \\\\
v_3(x_1) & v_3(x_2) & \\cdots & v_3(x_n) & 0 & 0 & 0 & \\ddots & 0 \\\\
\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\
v_m(x_1) & v_m(x_2) & \\cdots & v_m(x_n) & v_m(x_{n+1}) & 0 & 0 & \\cdots & v_m(x_d)
\\end{pmatrix} + 
\\begin{pmatrix} v_0(x_1) \\\\ v_0(x_2) \\\\ \\vdots \\\\ v_0(x_n) \\\\ v_0(x_{n+1}) \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}^T \\]

where:
\\begin{itemize}
    \\item Each \$v_i(x_j)\$ evaluates to the corresponding matrix entry at point \$x_j\$
    \\item The diagonal entries evaluate to 2 at their respective points
    \\item The output constraint column evaluates to \$(0,\\ldots,0,-3)\$ at point \$x_{n+1}\$
    \\item \$v_0(x)\$ interpolates the constant vector \$(\\delta_1,\\ldots,\\delta_n,3,0,\\ldots,0)\$
\\end{itemize}

For any boolean circuit \$C\$, we can construct an SSP instance \$(v_0(x),\\ldots,v_m(x),t(x))\$ such that \$C(x_1,\\ldots,x_\\ell) = 1\$ if and only if there exist values \$(a_{\\ell+1},\\ldots,a_m)\$ making the above polynomial division possible.

The correctness of this transformation follows from our construction: each gate's constraints are captured at distinct evaluation points, the boolean nature of our variables is enforced by the diagonal matrix, and the polynomial division condition.




Now, for notational convenience, we define \$v_0'(x) = v_0(x) - 1\$. 

At each evaluation point \$x_j\$, our circuit constraints require:
\\[ \\left(\\sum_{i=1}^m a_i v_i(x_j) + v_0'(x_j)\\right)^2 = 1 \\]

This set of point-wise constraints can be unified into a single polynomial constraint:
\\[ \\left(\\sum_{i=1}^m a_i v_i(x) + v_0'(x)\\right)^2 - 1 \\equiv 0 \\pmod{t(x)} \\]

where \$t(x) = \\prod_{j=1}^d (x - x_j)\$ is our target polynomial.

Equivalently, there must exist some polynomial \$h(x)\$ such that:
\\[ \\left(\\sum_{i=1}^m a_i v_i(x) + v_0'(x)\\right)^2 - 1 = h(x)t(x) \\]

This is precisely the SSP satisfiability condition. Thus, we have shown that circuit satisfiability is equivalent to the existence of coefficients \$a_i\$ that satisfy this polynomial divisibility condition.

\\subsection{From SSPs to SNARKs}

We'll use bilinear groups to construct our SNARK. Our construction begins with a bilinear group setup:
\\[ (\\mathbb{F}_p, G_1, G_2, G_T, e, g_1, g_2) \\]

The core idea of our construction leverages the pairing operation \$e\$ to verify polynomial equations in the exponent, roughly:
\\[ e(g_1^{[v_0(x) + \\sum_{i=1}^m a_i v_i(x)]}, g_2^{[v_0(x) + \\sum_{i=1}^m a_i v_i(x)]}) = e(g_1^{h(x)}, g_2^{t(x)}) \\cdot e(g_1, g_2) \\]
We cannot directly argue about polynomial in the exponents; \$x\$ is a formal variable not a concrete value. For this we will sample a uniformly random point \$\\tau\$ and check the polynomial equation in the exponent on \$\\tau\$. We will keep \$\\tau\$ secret from the prover thus, intuitively, from their point of view random.

The common reference string (CRS) forms the foundation of our construction. It consists of powers of a secret value \$\\tau\$:
\\[ \\text{CRS} = (bg, g_1^\\tau, g_1^{\\tau^2}, \\dots, g_1^{\\tau^d}, g_2^\\tau, g_2^{\\tau^2}, \\dots, g_2^{\\tau^d}, g_1^{\\beta v_{\\ell + 1}(\\tau)}, \\dots, g_1^{\\beta v_m(\\tau)}, h_2, h_2^\\beta) \\]

In our protocol, we distinguish between two types of values:
\\begin{itemize}
    \\item Statement: \$(a_1, \\dots, a_\\ell)\$ - the public inputs
    \\item Witness: \$(a_{\\ell+1}, \\dots, a_m)\$ - the private values
\\end{itemize}

For proof generation, the prover computes four crucial group elements:
\\begin{align*}
    \\pi_1 &= g_1^{\\sum_{i=\\ell+1}^m a_i v_i(\\tau)} \\\\
    \\pi_2 &= g_2^{\\sum_{i=\\ell+1}^m a_i v_i(\\tau)} \\\\
    \\pi_3 &= g_1^{h(z)} \\\\
    \\pi_4 &= g_1^{\\beta \\sum_{i=1}^m a_i v_i(\\tau)}
\\end{align*}

The verification process consists of two steps. First, the verifier computes intermediate values:
\\begin{align*}
    \\pi_1' &= g_1^{v_0(z) + \\sum_{i=1}^\\ell a_i v_i(\\tau)} \\cdot \\pi_1 \\\\
    \\pi_2' &= g_2^{v_0(z) + \\sum_{i=1}^\\ell a_i v_i(\\tau)} \\cdot \\pi_2
\\end{align*}

Then, the verifier performs three critical pairing checks:
\\begin{align*}
    e(\\pi_1', \\pi_2') &= e(\\pi_3, g_2^{t(\\tau)}) \\cdot e(g_1, g_2) \\\\
    e(\\pi_1, g_2) &= e(g_1, \\pi_2) \\\\
    e(\\pi_1, h_2^\\beta) &= e(\\pi_4, h_2)
\\end{align*}

These pairing equations serve distinct purposes in ensuring the proof's validity. The first check confirms that the SSP is satisfied by verifying the polynomial equation in the exponent. The second check ensures consistency between the prover's elements in groups \$G_1\$ and \$G_2\$. The final check prevents malformed group elements by verifying they were properly constructed using the CRS values.

\\subsection{Soundness}

Due to the non-interactive nature of our protocol, we cannot rely on black-box extraction for our security proofs. Instead, we require a non-black-box extractor with access to the prover's code, which is acceptable within our security model.

The soundness guarantees of SNARKs, including our SSP SNARK construction, differ from traditional cryptographic protocols. They necessarily rely on non-falsifiable assumptions or the random oracle model (ROM).

Non-falsifiable assumptions represent a unique class of cryptographic assumptions. The most widely used non-falsifiable assumptions are the knowledge-assumptions, informally the simplest form of these assumptions state that given \$(g, g^\\alpha)\$ no adversary can output a pair \$(g^x, (g^x)^\\alpha)\$ without actually ``knowing'' the exponent \$x\$. The term ``non-falsifiable'' stems from the fact that these assumptions make claims about what an adversary must know, rather than just what it can or cannot compute.

Our first attempt at formalizing soundness states that for every probabilistic polynomial-time (PPT) adversary \$\\mathcal{A}\$, there exists a PPT extractor \$\\mathcal{E}_\\mathcal{A}\$ such that:
\\[ \\Pr[(v_1, v_2) \\gets \\mathcal{A}(g, g^\\alpha, z), x \\gets \\mathcal{E}_\\mathcal{A}(g, g^\\alpha, z) : v_2 = v_1^\\alpha \\land v_1 \\neq g^x] = negl(\\lambda) \\]

This probability involves three key components:
\\begin{itemize}
    \\item \$z\$ represents auxiliary information available to the adversary
    \\item \$\\lambda\$ denotes the security parameter
    \\item \$negl(\\lambda)\$ represents a negligible function
\\end{itemize}

Recent research has revealed limitations in this initial formulation. Specifically, certain obfuscated programs, when provided as auxiliary input \$z\$, enable adversaries to produce valid pairs \$(v_1, v_2)\$ while preventing the extractor \$\\mathcal{E}_\\mathcal{A}\$ from determining \$x\$ due to the complexity of reverse-engineering the obfuscated program.

\\paragraph{Knowledge Soundness Definition}

To address these limitations, we've developed a more robust formulation for our SNARK construction that assumes only 'benign' auxiliary information. For all relations \$R \\in \\mathcal{R}\$ and non-uniform adversaries \$\\mathcal{A}\$, there exists a non-uniform PPT extractor \$\\mathcal{E}_\\mathcal{A}\$ such that for all benign \$z \\in \\mathcal{Z}\$:
\\[ \\begin{aligned}
&\\Pr[((v_1, v_2); c_0, \\dots, c_d) \\gets (\\mathcal{A} \\| \\mathcal{E}_\\mathcal{A})(bg, g_1, g_2, \\dots, g_1^{\\tau^d}, g_2^{\\tau^d}, z) : \\\\
&\\quad e(v_1, g_2) = e(g_1, v_2) \\land v_1 \\neq g_1^{\\sum_{i=0}^d c_i \\tau^i}] = negl(\\lambda)
\\end{aligned} \\]

This refined definition captures several crucial aspects: the adversary and extractor share access to the exact same inputs and random coins, the extractor must produce coefficients \$c_i\$ explaining the adversary's output, the auxiliary information must be ``benign'' (excluding problematic cases like obfuscated programs), and the probability of adversarial success without the extractor finding a valid witness must be negligible.

While this non-falsifiable assumption is stronger than traditional cryptographic assumptions, it appears to be necessary for constructing efficient SNARKs using current techniques.

\\paragraph{Knowledge Assumption}

We now formalize the knowledge assumption d-PKE (power knowledge of exponent).

For every non-uniform PPT adversary \$\\mathcal{A}\$ there exists a non-uniform PPT extractor \$\\mathcal{E}_\\mathcal{A}\$ such that for every ``benign'' \$z\$:

\$\$\\Pr [\\mathcal{A} || \\mathcal{E}_\\mathcal{A} (bg, g_1^\\tau, g_1^{\\tau^2}, \\dots, g_1^{\\tau^d}, g_2^\\tau, g_2^{\\tau^2}, \\dots, g_2^{\\tau^d}, z)\$\$
\$\$\\rightarrow (v_1, v_2; c_0, \\dots, c_d) : e(v_1, g_2) = e(g_1, v_2) \\land v_1 \\neq g_1^{\\sum_{i=0}^d c_i \\tau^i}] = negl(\\lambda)\$\$

This is basically saying that if the adversary can output a valid pair \$(v_1, v_2)\$ then the extractor can output a polynomial with coefficients \$c_0, \\dots, c_d\$ such that \$v_1 = g_1^{\\sum_{i=0}^d c_i \\tau^i}\$. Observe this assumption is very close to what we actually construct in the SNARK.

We now show a sketch of the knowledge soundness proof.

Assuming a knowledge-sound adversary \$\\mathcal{A}\$, we are going to construct \$\\mathcal{A}'\$ as follows:

\$\$\\mathcal{A}'(bg, \\{g_1^{\\tau^i}\\}_{i=0}^d, \\{g_2^{\\tau^i}\\}_{i=0}^d, z' = (z || \\{g_1^{\\beta v_i(\\tau)}\\}_{i=\\ell+1}^m, h_1, h_2^\\beta))\$\$

We give the rest of the CRS of the SNARK to the adversary inside of the auxiliary input \$z'\$. Then, this adversary runs the adversary of the knowledge soundness assumption as follows:

\$\$\\mathcal{A}(R, z, crs) \\rightarrow (\\pi_1, \\pi_2, \\pi_3, \\pi_4, (a_1, \\dots, a_\\ell))\$\$

such that the three pairing verification equation of the SNARK hold, and outputs \$v_1 = \\pi_1 \\cdot g_1^{v_0(\\tau) + \\sum_{i=1}^\\ell a_i v_i(\\tau)}\$, \$v_2 = \\pi_2 \\cdot g_2^{v_0(\\tau) + \\sum_{i=1}^\\ell a_i v_i(\\tau)}\$ where \$e(v_1, g_2) = e(g_1, v_2)\$. 

There is an extractor \$\\mathcal{E}_\\mathcal{A}'\$ that can output \$c_0, \\dots, c_d\$ such that \$v_1 = g_1^{\\sum_{i=0}^d c_i \\tau^i}\$.

We now construct the extractor \$\\mathcal{E}_\\mathcal{A}\$ for knowledge soundness (using the the extractor the knowledge assumption \$\\mathcal{E}_{\\mathcal{A}'}\$). The extractor is given \$v_1 = g_1^{f(\\tau)}\$ where \$f(x) = \\sum_{i=0}^d c_i x^i\$.

In order for this extractor to be able to extract the actual witness of the relation, we need two things:
\\begin{enumerate}
    \\item \$(f(x))^2 - 1\$ is divisible by \$t(x)\$, which can be reduced to the \$d\$-Target Group Strong Diffie-Hellman assumption.
    \\item \$f_{wit}(x) := f(x) - v_0(x) - \\sum_{i=1}^\\ell a_i v_i(x)\$ is in the span of \$\\{v_i\\}_{i=\\ell+1}^m\$ (equivalently there exist \$a_{\\ell+1}, \\dots, a_m\$ such that \$f_{wit}(x) = \\sum_{i=\\ell+1}^m a_i v_i(x)\$), which can be reduced to the \$d\$-Power Diffie-Hellman assumption.
\\end{enumerate}
The reductions are based on the fact that the pairing verification equations of the SNARK hold.

Since the two conditions are satisfied the extractor can output \$a_{\\ell+1}, \\dots, a_m\$ as a valid witness.

% \\let\\counterwithout\\relax
% \\let\\counterwithin\\relax

% \\let\\proof\\relax
% \\let\\endproof\\relax
% \\let\\example\\relax
% \\let\\endexample\\relax

% %% Tikz configuration
% \\usetikzlibrary{shapes.geometric, arrows}

% % Change thanks markers to numbers
% \\makeatletter
% \\let\\@fnsymbol\\@arabic
% \\makeatother

% %% LLNCS hyperref requirement
% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}

% %% Eurocrypt requirement
% \\pagestyle{plain}


% %% Allow align environment to span multiple pages
% \\allowdisplaybreaks

% %% Theorem Environments

% % Bold optional theorem titles
% \\makeatletter
% \\def\\th@definition{%
%   \\thm@notefont{}% same as heading font
%   \\normalfont % body font
% }
% \\makeatother

% % No italics
% % \\theoremstyle{definition}

% % Rename environments
% \\newtheorem{assumption}{Assumption}
% % \\newtheorem{construction}{Construction}
% \\newtheorem{attention}{Remark}
% \\newtheorem{notation}{Notation}

% % Algorithm counter to construction
% \\makeatletter 
% \\renewcommand\\thealgorithm{\\theconstruction.\\arabic{algorithm}} 
% \\@addtoreset{algorithm}{construction}
% \\makeatother

% %% Annotations
% \\newcommand\\todo[1]{\\textcolor{red}{TODO: #1}}
% \\definecolor{comment-color}{RGB}{68, 59, 141}
% \\renewcommand{\\algorithmiccomment}[1]{\\textcolor{comment-color}{// #1}}
% \\newcommand\\blue[1]{\\textcolor{blue}{#1}}
% \\newcommand{\\mathhl}[1]{\\colorbox{yellow}{\$\\displaystyle #1\$}}



% %% Diagram Macros

% \\newcommand{\\rightarr}[1]{\\sendmessageright{length=2.5cm, top=\\colorbox{white}{\$#1\$}, topstyle={yshift=-9}}}
% \\newcommand{\\leftarr}[1]{\\sendmessageleft{length=2.5cm, top=\\colorbox{white}{\$#1\$}, topstyle={yshift=-9}}}

% \\newcommand{\\cfbox}[2]{
%     \\colorlet{currentcolor}{.}
%     {\\color{#1}
%     \\fbox{\\color{currentcolor}#2}}
% }

\\newcommand{\\En}{\\mathcal{K}}
\\newcommand{\\Gn}{\\mathcal{G}}
\\newcommand{\\Po}{\\mathcal{P}}
\\newcommand{\\V}{\\mathcal{V}}


\\newcommand{\\niksdef}[6]{
For all expected polynomial-time adversaries 
\$\\mathcal{P}^*\$ 
there exists an expected polynomial-time extractor
\$\\mathcal{E}\$ such that
    \\[
    \\Pr_{\\mathsf{r}}
    \\left[
      \\begin{array}{l}
        #6
      \\end{array}
      \\middle\\vert
      \\begin{array}{l}
        % Generator
        \\mathsf{pp} \\gets \\mathcal{G}(1^{\\lambda}, N),\\\\ 
        % Adversarial Statement, Proof
        (#1, #2, #4) \\gets \\mathcal{P}^*(\\mathsf{pp}, \\mathsf{r}),\\\\
        % Key Generator
        (\\pk, \\vk) \\gets \\En(\\pp, #1),\\\\
        % Precondition
        #5,\\\\
        % Extractor
        #3 \\gets \\mathcal{E}(\\pp, \\mathsf{r})
      \\end{array}
    \\right]
    \\approx 
    1
    \\]
    where \$\\mathsf{r}\$ denotes an arbitrarily long random tape.
}


% \\newcommand{\\pp}{\\mathsf{pp}}
% \\newcommand{\\pk}{\\mathsf{pk}}
% \\newcommand{\\vk}{\\mathsf{vk}}

\\newcommand{\\R}{\\mathcal{R}}

\\newcommand{\\FP}{F'}
\\newcommand{\\io}{\\mathsf{x}}
\\newcommand{\\fu}{\\mathsf{u}}
\\newcommand{\\fw}{\\mathsf{w}}
\\newcommand{\\acc}{\\mathsf{U}}
\\newcommand{\\aw}{\\mathsf{W}}
\\newcommand{\\trivi}{\\fu_{\\bot}}
\\newcommand{\\trivw}{\\fw_{\\bot}}
\\newcommand{\\fold}{\\mathsf{NIFS}}
\\newcommand{\\snark}{\\mathsf{SNARK}}
\\newcommand{\\RIVC}{\\R_{\\mathsf{IVC}}}
\\newcommand{\\Str}{\\mathsf{s}}
\\newcommand{\\com}{\\mathsf{com}}

% \\newcommand{\\Gen}{\\mathsf{Gen}}
\\newcommand{\\Commit}{\\mathsf{Commit}}

% \\newcommand{\\negl}[1]{\\mathsf{negl}(#1)}

\\section{Recursive Proofs of Knowledge}

In this lecture, 
we discuss recursive zero-knowledge succinct non-interactive
arguments of knowledge (zkSNARKs).
%
We assume familiarity with zkSNARKs.

\\subsection{Introduction}

% Zero-Knowledge Proofs
Succinct non-interactive arguments of knowledge are short certificates that attest to the correct execution of a computation without revealing any secret inputs. 
%
Today, 
zero-knowledge proofs are being used 
to secure \\emph{billions} of dollars worth of assets~\\cite{zerocash, stark}.
%
Zero-knowledge proofs 
enable a new class of secure applications 
with enhanced integrity and privacy guarantees
such as verifiable databases~\\cite{zkvsql, vsql, accountablestorage, integridb},
private voting protocols~\\cite{privatevoting},
anonymous credentials~\\cite{cinderella, dacreds},
and
private cryptocurrencies~\\cite{zerocash, pinocchiocoin, stark}.

% Proving a Function
More technically, 
SNARKs 
(for circuit-satisfiability)
allow a prover to demonstrate that it knows a secret witness \$w\$
such that for prescribed circuit \$F\$ and prescribed input and output pair \$(x, y)\$ that \$F(x, w) = y\$.
%
% Proving Recursion
However, today, we will be interested in
proving \\emph{recursive} computation. 
Without loss of generality, we are interested in proving \\emph{tail}
recursion,
that is, we want to prove
(unbounded) recursive applications 
of a function \$F\$.
Unbounded recursion
in general allows us to implement more complex programming patterns such as 
\$\\mathsf{for}\$ and \$\\mathsf{while}\$, 
which are not bounded ahead of time.
This allows us to prove stateful computations with dynamic control flow.
%% Use cases
In practice, 
proving recursion allows us to recursively prove blockchain updates, verifiable delay functions, and even a universal machine,
where each recursive step is a single cycle of a CPU.


% Naive Solution
Historically, 
the best known approach to design a proof system for recursive applications of a function \$F\$ 
was to unroll the entire execution \$F \\circ F \\circ \\cdots \\circ F\$ into a monolithic arithmetic circuit,
and then use a standard proof system with succinct proofs for circuit satisfiability.
%
Unfortunately,
this would necessarily mean that the prover's space complexity would scale with the \\emph{entire trace} of the computation.
%
Moreover, 
in the setting of preprocessed arguments of knowledge
(where the prover and verifier would process the circuit into a succinct key to use for multiple inputs)
this fixed the recursion-depth ahead of time,
which often does not reflect practice.

% Recursive Solution Overview
The first breakthrough was due to Valiant~\\cite{valiant} in 2012, 
who proposed incrementally verifiable computation (IVC),
which reflected the recursive structure of the computation
into the proof itself:
%
Given a succinct proof \$\\pi_{i}\$ attesting to \$i\$ steps of computation,
the prover can write a succinct proof \$\\pi_{i + 1}\$ that attests to \$i + 1\$ steps
by proving the correct execution of an arithmetic circuit 
that runs the latest step of computation,
and checks \$\\pi_i\$ 
(using the proof system's verifier).
This avoids having to fix the recursion depth ahead of time,
while ensuring that the prover's space complexity only scales with a single step of execution.

Although undoubtedly elegant, 
Valiant's technique introduces a subtle issue:
Proofs of knowledge must satisfy a stronger notion of soundness 
known as \\emph{knowledge-soundness}.
%
Essentially,
a proof system is considered knowledge-sound 
if, 
for any successful prover with some secret input to the computation, 
there exists a corresponding \\emph{extractor} that,
with at most polynomial overhead,
can retrieve this secret input given access to the ``source code'' of the prover.
%
This extractor-based definition becomes problematic in the recursive setting:
Recursive proofs require \\emph{recursive extraction} 
in which the extractor for step \$i\$ 
plays the successful prover for the extractor at step \$i - 1\$. 
This incurs a polynomial blowup in the extractor for each successive recursive step. 
In particular, 
this results in a final extractor that runs in exponential-time with respect to the recursion-depth, 
which disqualifies it as a valid extractor. 
%
To account for this issue, 
Valiant's original technique 
(and modern techniques) 
can only provably guarantee logarithmic-depth recursion in standard models.

\\subsection{Preliminaries}

We operate in the \\textit{preprocessing model}, which means that a trusted party will be responsible for generating a prover and verifier key.

% Definition
\\begin{definition}
   [Incrementally Verifiable Computation]\\label{def:ivc}
   An 
   incrementally verifiable computation (IVC)  
   scheme is defined by
   PPT algorithms 
   \$(\\mathcal{G}, \\mathcal{P}, \\mathcal{V})\$ 
   and deterministic \$\\En\$
   denoting the generator, 
   the prover, 
   the verifier,
   and the encoder respectively,
   with the following interface
   \\begin{itemize}
     \\item \$\\mathcal{G}(1^\\lambda, N) \\to \\pp\$: 
     on input security parameter \$\\lambda\$ and size bounds \$N\$, 
     samples public parameters \$\\pp\$.
     \\item \$\\En(\\pp, F) \\to (\\pk, \\vk)\$: 
     on input public parameters \$\\pp\$, 
     and polynomial-time function \$F\$,
     deterministically produces
     a prover key \$\\pk\$ 
     and a verifier key \$\\vk\$.
     \\item \$\\mathcal{P}(\\pk, (i, z_0, z_{i}), \\omega_{i}, \\pi_{i}) \\to \\pi_{i+1}\$: 
     on input a prover key \$\\pk\$, 
     a counter \$i\$, 
     an initial input \$z_0\$, 
     a claimed output after \$i\$ iterations \$z_i\$,
     a non-deterministic advice \$\\omega_i\$,
     and an IVC proof \$\\pi_i\$ attesting to \$z_i\$,
     produces a new proof \$\\pi_{i + 1}\$ attesting to \$z_{i + 1} = F(z_{i}, \\omega_{i})\$.
     \\item \$\\mathcal{V}(\\vk, (i, z_0, z_{i}), \\pi_{i}) \\to \\{0, 1\\}\$: 
     on input a verifier key \$\\vk\$,
     a counter \$i\$,
     an initial input \$z_0\$, 
     a claimed output after \$i\$ iterations \$z_i\$,
     and an IVC proof \$\\pi_i\$ attesting to \$z_i\$,
     outputs \$1\$ if \$\\pi_i\$ is accepting, and 
     \$0\$ otherwise.
   \\end{itemize}
     An IVC scheme 
     \$(\\mathcal{G}, \\En, \\mathcal{P}, \\mathcal{V})\$
     satisfies the following requirements.
     \\begin{enumerate}
     \\item Perfect Completeness:    
     For any
     PPT adversary \$\\mathcal{A}\$
     \\begin{equation*}
     \\Pr
     \\left[
       \\begin{array}{l}
         \\mathcal{V}(\\vk, (i + 1, z_0, z_{i + 1}), \\pi_{i + 1}) = 1 
       \\end{array}
       \\middle\\vert
       \\begin{array}{l}
         \\mathsf{pp} \\gets \\mathcal{G}(1^{\\lambda}, N),\\\\
         (F, (i, z_0, z_i), (\\omega_i, \\pi_i)) \\gets \\mathcal{A}(\\mathsf{pp}),\\\\
         (\\pk, \\vk) \\gets \\En(\\pp, F),\\\\
         z_{i + 1} \\gets F(z_{i}, \\omega_{i}),\\\\
         \\mathcal{V}(\\vk, i, z_0, z_{i}, \\pi_{i}) = 1,\\\\
         \\pi_{i + 1} \\gets \\mathcal{P}(\\pk, (i, z_0, z_i), (\\omega_{i}, \\pi_{i}))
       \\end{array}
     \\right] = 1
     \\end{equation*}
     where \$F\$ is a polynomial-time computable function represented as an arithmetic circuit.
   \\item Knowledge Soundness:
   Consider constant \$n \\in \\mathbb{N}\$.

   \\niksdef
   % Structure
   {F}
   % Statement
   {(z_0, z_i)}
   % Witness
   {(\\omega_0, \\ldots, \\omega_{n - 1})}
   % Proof Format
   {\\Pi}
   % Precondition
   {\\mathcal{V}(\\vk, (n, z_0, z), \\Pi) = 1}
   % Postcondition
   {z_n = z \\text{ where }\\\\ z_{i + 1} \\gets F(z_i, \\omega_i)\\\\ \\forall i \\in \\{0, \\ldots, n - 1\\}}
   
   Moreover, 
     \$F\$ is a polynomial-time computable function represented as an arithmetic circuit.
     \\item Succinctness: 
     The size of an IVC proof \$\\pi\$ is independent of the number of iterations \$i\$.
   \\end{enumerate}
\\end{definition}

\\begin{definition}[Non-Interactive Argument of Knowledge]\\label{def:nark}
   Consider a relation \$\\R\$ over 
   public parameters, structure, instance, and witness tuples.
   A non-interactive argument of knowledge for \$\\R\$ consists of PPT algorithms
   \$(\\Gn, \\Po,\\V)\$ 
   and deterministic \$\\En\$,
   denoting the generator, 
   the prover,    
   the verifier
   and the encoder respectively with the following interface.
   \\begin{itemize}
       \\item \$\\Gn(1^{\\lambda}, N) \\to \\pp\$: 
       On input security parameter \$\\lambda\$,
       and length parameter \$N\$
       samples public parameters \$\\pp\$.
       \\item \$\\En(\\pp, \\Str) \\to (\\pk, \\vk)\$: 
       On input structure \$\\Str\$, 
       representing common structure among instances,
       outputs the prover key \$\\pk\$ and verifier key \$\\vk\$.
       \\item \$\\Po(\\pk, u, w) \\to \\pi\$: On input instance \$u\$ and
         witness \$w\$, outputs a proof \$\\pi\$ proving that \$(\\pp, \\Str, u, w) \\in \\R\$.
       \\item \$\\V(\\vk, u, \\pi) \\to \\{0, 1\\}\$: 
       Checks instance \$u\$ 
       given proof \$\\pi\$.
   \\end{itemize}
   An argument of knowledge satisfies \\textit{completeness} if for any PPT adversary \$\\mathcal{A}\$,
   \\begin{align*}
   \\Pr
   \\left[
       \\begin{array}{l}
       \\V(\\vk, u, \\pi) = 1
       \\end{array}
       \\middle\\vert
       \\begin{array}{l}
       \\pp \\gets \\Gn(1^{\\lambda}, N),\\\\
       (\\Str,(u, w)) \\gets \\mathcal{A}(\\pp),\\\\
       (\\pp, \\Str, u, w) \\in \\R,\\\\
       (\\pk, \\vk) \\gets \\En(\\pp, \\Str),\\\\
       \\pi \\gets \\Po(\\pk, u, w)
       \\end{array}
       \\right]
   = 1.
   \\end{align*}
   An argument of knowledge satisfies \\textit{knowledge soundness} if for all PPT adversaries \$\\Po^*\$ there exists a PPT extractor \$\\mathcal{E}\$ such that for all randomness \$\\mathsf{r}\$
   \\[
   \\Pr
   \\left[
       \\begin{array}{l}
       (\\pp, \\Str, u, w) \\in \\R
       \\end{array}
       \\middle\\vert
       \\begin{array}{l}
       \\pp \\gets \\Gn(1^{\\lambda}, N),\\\\
       (\\Str,u,\\pi) \\gets \\Po^*(\\pp, \\mathsf{r}),\\\\
       (\\pk, \\vk) \\gets \\En(\\pp, \\Str),\\\\
       \\V(\\vk, u, \\pi) = 1,\\\\
       w \\gets \\mathcal{E}(\\pp, \\mathsf{r})
       \\end{array}
       \\right]
   \\approx 1.
   \\]
\\end{definition}

\\begin{definition}[Succinctness]
   A non-interactive argument system is succinct if the size of the proof \$\\pi\$
   is polylogarithmic in the size of the witness \$w\$.
\\end{definition}

\\begin{definition}[Commitment Scheme]\\label{def:commitment}
   A commitment scheme is defined by polynomial-time algorithm
   \$\\Gen : \\mathbb{N}^2 \\to P\$
   that produces public parameters given the security parameter and size parameter, 
   a deterministic polynomial-time algorithm
   \$\\Commit : P \\times M \\times R \\to C\$
   that produces a commitment in \$C\$ given a public parameters, message, and randomness tuple
   such that 
   binding holds.
   That is, 
   for any \$\\mathsf{PPT}\$ adversary \$\\mathcal{A}\$,
   given
   \$\\pp \\gets \\Gen(\\lambda, n)\$,
   and given \$((m_1, r_1), (m_2, r_2)) \\gets \\mathcal{A}(\\pp)\$
   we have that
   \\[
       \\Pr[(m_1, r_1) \\neq (m_2, r_2) \\land \\Commit(\\pp, m_1, r_1) = \\Commit(\\pp, m_2, r_2)] \\approx 0.
   \\]
   %
   The commitment scheme is deterministic if \$\\Commit\$ does not use its randomness. 
 \\end{definition}

 \\begin{definition}[Circuit Satisfiability]
   We define the circuit satisfiability relation \$\\mathsf{CSAT}\$
   over structure, instance, witness tuples
   as follows.
   \\begin{equation*}
     \\mathsf{CSAT}
     = 
     \\left\\{
     \\begin{array}{l}
         % Statment, Witness
         (C, (x, y), w)
     \\end{array}
     \\middle\\vert
     \\begin{array}{l}
         C(x, w) = y
     \\end{array}
     \\right\\}.
   \\end{equation*}  
 \\end{definition}

\\subsection{Construction}

% Formal Construction

\\begin{construction}[Incrementally Verifiable Computation]\\label{cons:ivc}
   Given a 
   a succinct commitment scheme
   \$(\\Gen, \\Commit)\$
   and a
   succinct non-interactive argument of knowledge 
   \$\\snark\$ for circuit-satisfiability
   we construct an IVC scheme as follows.
   
   %% Function F  
   Consider an arithmetic circuit \$F\$ 
   that takes non-deterministic input
   We begin by defining an augmented circuit
   \$\\FP\$
   as follows,
   where all input arguments are taken as non-deterministic advice.
   %
   \\begin{mdframed}[nobreak=true]
     \\noindent \\underline{\$\\FP(\\io_i, \\omega_i, \\pi_i)\$}:
     \\begin{enumerate}
       \\item Parse \$(\\vk_\\snark, i, z_0, z_i) \\gets \\io_i\$.
       \\item  If \$i = 0\$:
       \\begin{enumerate}
         \\item Check that \$z_0 = z_i\$.
         \\label{ivc:fp:base}
       \\end{enumerate}
       \\item Otherwise:
       \\begin{enumerate} 
         \\item Check that \$\\snark.\\V(\\vk_\\snark, \\io_i, \\pi_i) = 1\$.
         \\label{ivc:fp:check:general}
       \\end{enumerate}
       \\item Output \$\\io_{i + 1} \\gets (\\vk_{\\snark}, i + 1, z_0, F(z_i, \\omega_i))\$.
       \\label{ivc:fp:output}
     \\end{enumerate}  
   \\end{mdframed} 
   %
   %
   Given the augmented circuit \$\\FP\$,
   we define \$(\\Gn, \\En, \\Po, \\V)\$ as follows.
   \\begin{mdframed}[nobreak=true]
     \\underline{\$\\Gn(\\lambda, N)\$}:
     \\begin{enumerate}
       \\item Output \$\\pp \\gets \\snark.\\mathcal{G}({\\lambda}, N)\$.
     \\end{enumerate}
   \\end{mdframed}
   %
   \\begin{mdframed}[nobreak=true]
     \\underline{\$\\En(\\pp, F)\$}:
     \\begin{enumerate}
       
       \\item Compute 
       \$(\\pk_\\snark, \\vk_\\snark) \\gets \\snark.\\En(\\pp, \\FP)\$.
       \\item Output \$\\pk \\gets (\\FP, \\pk_\\snark, \\vk_\\snark)\$ and \$\\vk \\gets \\vk_\\snark\$.
     \\end{enumerate}
   \\end{mdframed}
   \\begin{mdframed}[nobreak=true]
     \\underline{\$\\Po(\\pk, (i, z_0, z_i), (\\omega_i, \\pi_i))\$}:
     \\begin{enumerate} 
       \\item Parse \$(\\FP, \\pk_\\snark, \\vk_\\snark) \\gets \\pk\$.
       \\item Compute
       \$\\io_{i + 1} \\gets \\FP(\\vk_{\\snark},
       (i, z_0, z_i), \\omega_i, \\pi_i)\$.
       \\item Let \$\\io_i \\gets (\\vk_{\\snark}, i, z_0, z_i)\$
       \\label{ivc:prover:io}
       \\item Output
       \$
       \\pi_{i + 1} \\gets \\snark.\\Po(\\pk_\\snark, (\\bot, \\io_{i + 1}), (\\io_i
       , \\omega_i, \\pi_i))
       \$.
       \\label{ivc:prover:proof}
     \\end{enumerate} 
   \\end{mdframed}
   %
   \\begin{mdframed}[nobreak=true]
     \\underline{\$\\V(\\vk, (i, z_0, z_i), \\pi_i)\$}:
     \\begin{enumerate}
       \\item If \$i = 0\$: Check that \$z_i = z_0\$.
       \\label{ivc:verifier:base}
       \\item Otherwise:
       \\begin{enumerate}
         \\item Parse \$\\vk_{\\snark} \\gets \\vk\$.
         \\item Let \$\\io_i \\gets (\\vk_\\snark, i, z_0, z_i)\$.
         \\label{ivc:v:check:first}
         \\item Check that
         \$\\snark.\\V(\\vk_\\snark, (\\bot, \\io_i), \\pi_i) = 1\$.
         \\label{ivc:v:check:second}
       \\end{enumerate}
       
     \\end{enumerate}
   \\end{mdframed}
 \\end{construction}

\\begin{lemma}[Completeness]
 Construction~\\ref{cons:ivc}
 is complete.
\\end{lemma}
% \\newcommand{\\proof}{\\noindent{\\bf Proof. }} %% To begin a proof write \\proof

\\begin{proof}
   % Adversary
 Consider arbitrary PPT adversary \$\\mathcal{A}\$.
 % public parameters
 Suppose \$\\pp \\gets \\Gen(1^{\\lambda}, N)\$.
 % Structure, Instance, Witness
 Suppose that
 \\[
 (F, (z_0, z_i, i), \\pi_i) \\gets \\mathcal{A}(\\pp).
 \\]
 % Prover Key, Verifier Key
 Suppose that for
   \$(\\pk, \\vk) \\gets \\En(\\pp, F)\$
 % Precondition
 we have that
 \\begin{align}\\label{ivc:completeness:precondition:verifier}
   \\V(\\vk, (z_0, z_{i}, i), \\pi_{i}) = 1.
 \\end{align}
 % Postcondition
 Then,
 given
 \\begin{align*}
   z_{i + 1} \\gets F(z_{i}, \\omega_{i})
 \\end{align*}
 and 
 \\begin{align*}
   \\pi_{i + 1} \\gets \\Po(\\pk, (z_0, z_i, i), (\\omega_i, \\pi_i))
 \\end{align*}
 we must show that
 \\begin{align}\\label{ivc:completeness:postcondition}
   \\V(\\vk, (z_0, z_{i+1}, N), \\pi_{i + 1}) = 1
 \\end{align}
 with probability \$1\$.
 %
 
 % Case i = 0
 Indeed, 
 consider the base case where \$i = 0\$.
 %
 Then, 
 by Precondition~\\ref{ivc:completeness:precondition:verifier},
 by the verifier's check in the base case 
 (Step~\\ref{ivc:verifier:base})
 we have that \$z_0 = z_i\$.
 %
 Therefore,
 \$\\Po\$ can successfully compute \$\\io_{i + 1}\$ 
 (Step~\\ref{ivc:prover:io}),
 because the base case check of \$\\FP\$ 
 (Step~\\ref{ivc:fp:base}) passes.
 %
 Then,
 by construction of \$\\FP\$
 (Step~\\ref{ivc:fp:output})
 we have that 
 \\[
   \\io_{i + 1} = (\\vk_{\\snark}, i + 1, z_0, F(z_i, \\omega_i))
 \\]
 Moreover,
 by the completeness
 of \$\\snark\$,
 we have that \$\\pi_{i + 1}\$ generated by \$\\Po\$ 
 (Step~\\ref{ivc:prover:proof})
 is indeed satisfying.
 %
 Therefore, 
 both the checks of \$\\V\$ in Steps~\\ref{ivc:v:check:first} and~\\ref{ivc:v:check:second}
 are passing.
 %
 As such,
 we have that postcondition~\\ref{ivc:completeness:postcondition}
 holds.
 
 
 % Case i \\geq 1
 Suppose instead that \$i \\geq 1\$.
 %
 by Precondition~\\ref{ivc:completeness:precondition:verifier},
 by the verifier's check in the general case 
 we have that
 \\begin{align*}
   \\snark.\\V(\\vk_\\snark, (\\bot, \\io_i), \\pi_i) = 1
 \\end{align*}
 for \$\\io_i = (\\vk_\\snark, i, z_0, z_i)\$.
 %
 Then,
 \$\\Po\$ can successfully compute \$\\io_{i + 1}\$ 
 (Step~\\ref{ivc:prover:io}),
 as the SNARK verifier check in \$\\FP\$
 (Step~\\ref{ivc:fp:check:general})
 holds.
 %
 %
 Once again,
 by construction of \$\\FP\$
 (Step~\\ref{ivc:fp:output})
 we have that 
 \\[
   \\io_{i + 1} = (\\vk_{\\snark}, i + 1, z_0, F(z_i, \\omega_i))
 \\]
 Moreover,
 by the completeness
 of \$\\snark\$,
 we have that \$\\pi_{i + 1}\$ generated by \$\\Po\$ 
 (Step~\\ref{ivc:prover:proof})
 is indeed satisfying.
 %
 Therefore, 
 both the checks of \$\\V\$ in Steps~\\ref{ivc:v:check:first} and~\\ref{ivc:v:check:second}
 are passing.
 %
 As such,
 we have that postcondition~\\ref{ivc:completeness:postcondition}
 holds.
\\end{proof}

\\begin{lemma}[Knowledge-Soundness]
 Construction~\\ref{cons:ivc}
 is knowledge-sound.
\\end{lemma}

% Proof for ivc knowledge soundness

\\begin{proof}
   % Premise
   Let \$n\$ be a global constant.
   %
   Consider a deterministic expected polynomial-time adversary \$\\Po^*\$.
   %
   Let \$\\pp \\gets \\Gen(1^{\\lambda}, N)\$.
   %
   Suppose 
   on input \$\\pp\$ and randomness \$\\mathsf{r}\$,
   \$\\Po^*\$
   outputs 
   polynomial-time function \$F\$,
   instance \$(z_0, z)\$,
   and IVC proof \$\\pi\$. 
   %
   Let \$(\\pk, \\vk) \\gets \\En(\\pp, F)\$.
   Suppose that
   % Precondition
   \\begin{align*}
     \\V(\\vk, (z_0, z, n), \\pi) = 1.
   \\end{align*}
   % Postcondition
   We must construct an expected polynomial-time extractor \$\\mathcal{E}\$
   that, 
   with input \$(\\pp, \\mathsf{r})\$,
   outputs \$(\\omega_0, \\ldots, \\omega_{n - 1})\$ 
   such that by computing
   \\begin{align*}
     z_{i + 1} \\gets F(z_{i}, \\omega_{i})
   \\end{align*}
   we have that \$z_n = z\$ with probability 
   \$1 - \\negl{\\lambda}\$.
   
   % Overview
   We show inductively that we can construct an expected polynomial-time extractor \$\\mathcal{E}_i\$ that outputs 
   \$((z_i, \\ldots, z_{n - 1}), (\\omega_i, \\ldots, \\omega_{n - 1}), \\pi_i)\$ such that for all \$j \\in  \\{i + 1, \\ldots, n\\}\$,
   \\begin{align*}
     z_j = F(z_{j - 1}, \\omega_{j - 1})
   \\end{align*}
   and 
   \\begin{align}\\label{eq:ih:2}
     \\V(\\vk, z_0, z_i, \\pi_i) = 1
   \\end{align}
   for \$z_n = z\$ with probability \$1 - \\negl{\\lambda}\$.
   %
   Then, 
   because when \$i = 0\$,
   \$\\V\$ checks that \$z_0 = z_i\$ 
   the values \$(\\omega_0, \\ldots, \\omega_{n - 1})\$ 
   retrieved by \$\\mathcal{E} = \\mathcal{E}_0\$
   are such that computing  
   \$z_{i + 1} = F(z_i, \\omega_i)\$ for all \$i \\geq 1\$ gives \$z_n = z\$.
   
   At a high level,
   to construct an extractor \$\\mathcal{E}_{i - 1}\$,
   we first assume the existence of \$\\mathcal{E}_i\$ that satisfies the inductive hypothesis. 
   We then use \$\\mathcal{E}_i\$ to construct an adversary 
   for the underlying succinct non-interactive argument, which we denote as \$\\widetilde{\\Po}_{i - 1}\$.
   This in turn guarantees an extractor for the underlying non-interactive argument,
   which we denote as \$\\widetilde{\\mathcal{E}}_{i - 1}\$. 
   We then use \$\\widetilde{\\mathcal{E}}_{i - 1}\$ to construct \$\\mathcal{E}_{i - 1}\$ that satisfies the inductive hypothesis.
   
   
   % Base case
   In the base case,
   let \$\\mathcal{E}_n(\\pp, \\mathsf{r})\$ 
   output 
   \$(\\bot, \\bot, \\pi_n)\$ 
   where \$\\pi_n\$ is the output of 
   \$\\Po^*(\\pp, \\mathsf{r})\$.
   By the premise,
   we have that \$\\pi_n\$ is satisfying.
   As such,
   \$\\mathcal{E}_n\$ succeeds with probability \$1 - \\negl{\\lambda}\$ in expected polynomial-time.
   
   % Inductive Step
   For \$i \\geq 1\$, 
   suppose we can construct an expected polynomial-time extractor 
   \$\\mathcal{E}_i\$ 
   that outputs
   \$((z_i, \\ldots, z_{n - 1}), (\\omega_i, \\ldots, \\omega_{n - 1}))\$, 
   and \$\\pi_i\$ 
   that satisfies the inductive hypothesis.
   % Construct SNARK prover
   To construct an extractor \$\\mathcal{E}_{i - 1}\$, 
   we first construct an adversary \$\\widetilde{\\Po}_{i - 1}\$
   for the underlying SNARK as follows.
   %
   %
   %
   \\begin{mdframed}[nobreak=true]
     \\noindent \\underline{\$\\widetilde{\\Po}_{i - 1}(\\pp, \\mathsf{r})\$}: 
     \\begin{enumerate}
       \\item Let \$(F, z_0) \\gets \\Po^*(\\pp, \\mathsf{r})\$
       \\item Let \$((z_i, \\ldots, z_{n - 1}), (\\omega_i, \\ldots, \\omega_{n -
       1}), \\pi_i) \\gets \\mathcal{E}_i(\\pp, \\mathsf{r})\$.
       \\item Let \$\\vk_{\\snark} \\gets \\snark.\\En(\\pp, F)\$.
       \\item Let \$\\io_i \\gets (\\vk_\\snark, (i, z_0, z_i))\$.
       \\item Output \$(\\FP, (\\bot, \\io_i), \\pi_i)\$.
     \\end{enumerate}
   \\end{mdframed}
   
   % SNARK prover success probability
   We now analyze the success probability of \$\\widetilde{\\Po}_{i - 1}\$. 
   By the inductive hypothesis,
   we have that 
   \\[
   \\V(\\vk, z_0, z_i, \\pi_i) = 1,
   \\]
   where 
   \$\\pi_i \\gets \\mathcal{E}_i(\\pp, \\mathsf{r})\$ 
   with probability 
   \$1 - \\negl{\\lambda}\$.
   Therefore, 
   by the the verifier's checks 
   we have that
   \\[
   \\snark.\\V(\\vk_\\snark, (\\bot, \\io_i), \\pi_i) = 1
   \\]
   for \$\\io_i = (\\vk_\\snark, (i, z_0, z_i))\$
   and \$\\vk_\\snark \\gets \\snark.\\En(\\pp, \\FP)\$.
   Therefore,
   \$\\widetilde{\\Po}_{i - 1}\$
   succeeds in producing a satisfying proof \$\\pi_i\$
   for structure \$\\FP\$ and instance \$\\io_i\$
   with probability \$1 - \\negl{\\lambda}\$.
 
   
   % Corresponding SNARK extractor
   Then,
   by the knowledge soundness of \$\\snark\$
   there exists an 
   expected-polynomial-time
   extractor \$\\widetilde{\\mathcal{E}}_{i - 1}\$ that
   outputs 
   \$(\\io_{i - 1}, \\omega_{i - 1}, \\pi_{i - 1})\$
   such that
   \$\\io_i = \\FP(\\io_{i - 1}, \\omega_{i - 1}, \\pi_{i - 1})\$
   with probability \$1 - \\negl{\\lambda}\$.
   
   % Constructing E_{i - 1}
   Given \$\\widetilde{\\mathcal{E}}_{i - 1}\$, 
   we construct an expected polynomial time \$\\mathcal{E}_{i - 1}\$ as follows.
 
 
   \\begin{mdframed}[nobreak=true]
   \\noindent \\underline{\$\\mathcal{E}_{i - 1}(\\pp, \\mathsf{r})\$}: 
   \\begin{enumerate}
     \\item Run \$\\widetilde{\\Po}_{i - 1}(\\pp, \\mathsf{r})\$
     to parse
     \\[
     ((z_i, \\ldots, z_{n - 1}), (\\omega_i, \\ldots, \\omega_{n -
     1}), \\pi_i)
     \\]
     from its internal state.
     \\item Compute \$(\\io_{i - 1}, \\omega_{i - 1}, \\pi_{i - 1}) \\gets \\widetilde{\\mathcal{E}}_{i - 1}(\\pp, \\mathsf{r})\$.
     \\item Parse \$(\\vk_\\snark, (i - 1, z_0, z_{i - 1})) \\gets \\io_{i - 1}\$
     \\item Output \$((z_{i - 1}, \\ldots, z_{n - 1}), (\\omega_{i - 1}, \\ldots, \\omega_{n - 1}), \\pi_{i - 1})\$. 
   \\end{enumerate}
 \\end{mdframed}
 
  We now reason about the success probability of \$\\mathcal{E}_{i - 1}\$.
   We first reason that the output \$(z_{i - 1}, \\ldots, z_{n - 1})\$, and
   \$(\\omega_{i - 1}, \\ldots, \\omega_{n - 1})\$ are valid.
   By the inductive hypothesis, 
   we already have that for all \$j \\in \\{i + 1, \\ldots, n\\}\$,
   \\begin{align*}
     z_j = F(z_{j - 1}, \\omega_{j - 1})
   \\end{align*}
   with probability \$1 - \\negl{\\lambda}\$.
   Moreover,
   by the success probability of \$\\mathcal{E}_{i - 1}\$,
   we have that
   \\begin{align*}
     z_{i} = F(z_{i - 1}, \\omega_{i - 1})
   \\end{align*}
   and that
   \\[
   \\snark.\\V(\\vk_\\snark, \\io_{i - 1}, \\pi_{i - 1}) = 1
   \\]
   where \$\\io_{i - 1} = (\\vk_\\snark, (i - 1, z_0, z_i))\$
   with probability \$1 - \\negl{\\lambda}\$.
   %
   Therefore we have that \$\\mathcal{E}_{i - 1}\$
   succeeds with probability \$1 - \\negl{\\lambda}\$
   satisfying the inductive hypothesis.
 \\end{proof} 

